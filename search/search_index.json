{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Patterns and Techniques for Writing High-Performance Applications with Go","text":"<p>The Go App Optimization Guide is a collection of technical articles aimed at helping developers write faster, more efficient Go applications. Whether you're building high-throughput APIs, microservices, or distributed systems, this series offers practical patterns, real-world use cases, and low-level performance insights to guide your optimization efforts.</p> <p>While Go doesn\u2019t expose as many knobs for performance tuning as languages like C++ or Rust, it still provides plenty of opportunities to make your applications significantly faster. From memory reuse and allocation control to efficient networking and concurrency patterns, Go offers a pragmatic set of tools for writing high-performance code.</p> <p>We focus on concrete techniques with measurable impact you can apply immediately\u2014covering everything from core language features to advanced networking strategies.</p>"},{"location":"#whats-covered-so-far","title":"What\u2019s Covered So Far","text":""},{"location":"#common-go-patterns-for-performance","title":"Common Go Patterns for Performance","text":"<p>In this first article, we explore a curated set of high-impact performance patterns every Go developer should know:</p> <ul> <li>Using <code>sync.Pool</code> effectively</li> <li>Avoiding unnecessary allocations</li> <li>Struct layout and memory alignment</li> <li>Efficient error handling</li> <li>Zero-cost abstractions with interfaces</li> <li>In-place sorting and slices reuse</li> </ul> <p>Each pattern is grounded in practical use cases, with benchmarks and examples you can copy into your own codebase.</p>"},{"location":"#whats-coming-next","title":"What\u2019s Coming Next","text":"<p>Info</p> <p>WIP is under networking-go branch.</p>"},{"location":"#high-performance-networking-in-go","title":"High-Performance Networking in Go","text":"<p>In our upcoming deep dive into networking, we'll focus on building high-throughput network services with Go\u2019s standard library and beyond. This includes:</p> <ul> <li>Efficient use of <code>net/http</code> and <code>net.Conn</code></li> <li>Managing large volumes of concurrent connections</li> <li>Performance tuning with epoll/kqueue and <code>GOMAXPROCS</code></li> <li>Load testing techniques and bottleneck diagnostics</li> <li>TBD...</li> </ul> <p>We'll also explore when to drop down to lower-level libraries like <code>fasthttp</code>, and how to balance performance with maintainability.</p>"},{"location":"#who-this-is-for","title":"Who This Is For","text":"<p>This series is ideal for:</p> <ul> <li>Backend engineers optimizing Go services in production</li> <li>Developers working on latency-sensitive systems</li> <li>Teams migrating to Go and building performance-critical paths</li> <li>Anyone curious about Go\u2019s performance model and trade-offs</li> </ul> <p>Stay tuned\u2014more articles, code samples, and tools are on the way. You can bookmark this page to follow the series as it evolves.</p>"},{"location":"01-common-patterns/","title":"Common Go Patterns for Performance","text":"<p>Optimizing Go applications requires understanding common patterns that help reduce latency, improve memory efficiency, and enhance concurrency. This guide organizes 15 key techniques into four practical categories.</p>"},{"location":"01-common-patterns/#memory-management-efficiency","title":"Memory Management &amp; Efficiency","text":"<p>These strategies help reduce memory churn, avoid excessive allocations, and improve cache behavior.</p> <ul> <li> <p>Object Pooling   Reuse objects to reduce GC pressure and allocation overhead.</p> </li> <li> <p>Memory Preallocation   Allocate slices and maps with capacity upfront to avoid costly resizes.</p> </li> <li> <p>Struct Field Alignment   Optimize memory layout to minimize padding and improve locality.</p> </li> <li> <p>Avoiding Interface Boxing   Prevent hidden allocations by avoiding unnecessary interface conversions.</p> </li> <li> <p>Zero-Copy Techniques   Minimize data copying with slicing and buffer tricks.</p> </li> <li> <p>Memory Efficiency and Go\u2019s Garbage Collector   Reduce GC overhead by minimizing heap usage and reusing memory.</p> </li> <li> <p>Stack Allocations and Escape Analysis   Use escape analysis to help values stay on the stack where possible.</p> </li> </ul>"},{"location":"01-common-patterns/#concurrency-and-synchronization","title":"Concurrency and Synchronization","text":"<p>Manage goroutines, shared resources, and coordination efficiently.</p> <ul> <li> <p>Goroutine Worker Pools   Control concurrency with a fixed-size pool to limit resource usage.</p> </li> <li> <p>Atomic Operations and Synchronization Primitives   Use atomic operations or lightweight locks to manage shared state.</p> </li> <li> <p>Lazy Initialization (<code>sync.Once</code>)   Delay expensive setup logic until it's actually needed.</p> </li> <li> <p>Immutable Data Sharing   Share data safely between goroutines without locks by making it immutable.</p> </li> <li> <p>Efficient Context Management   Use <code>context</code> to propagate timeouts and cancel signals across goroutines.</p> </li> </ul>"},{"location":"01-common-patterns/#io-optimization-and-throughput","title":"I/O Optimization and Throughput","text":"<p>Reduce system call overhead and increase data throughput for I/O-heavy workloads.</p> <ul> <li> <p>Efficient Buffering   Use buffered readers/writers to minimize I/O calls.</p> </li> <li> <p>Batching Operations   Combine multiple small operations to reduce round trips and improve throughput.</p> </li> </ul>"},{"location":"01-common-patterns/#compiler-level-optimization-and-tuning","title":"Compiler-Level Optimization and Tuning","text":"<p>Tap into Go\u2019s compiler and linker to further optimize your application.</p> <ul> <li> <p>Leveraging Compiler Optimization Flags   Use build flags like <code>-gcflags</code> and <code>-ldflags</code> for performance tuning.</p> </li> <li> <p>Stack Allocations and Escape Analysis   Analyze which values escape to the heap to help the compiler optimize memory placement.</p> </li> </ul>"},{"location":"01-common-patterns/atomic-ops/","title":"Atomic Operations and Synchronization Primitives","text":"<p>In high-concurrency systems, performance isn't just about what you do\u2014it's about what you avoid. Lock contention, cache line bouncing and memory fences quietly shape throughput long before you hit your scaling ceiling. Atomic operations are among the leanest tools Go offers to sidestep these pitfalls.</p> <p>While Go provides the full suite of synchronization primitives, there's a class of problems where locks feel like overkill. Atomics offers clarity and speed for low-level coordination\u2014counters, flags, and simple state machines, especially under pressure.</p>"},{"location":"01-common-patterns/atomic-ops/#understanding-atomic-operations","title":"Understanding Atomic Operations","text":"<p>Atomic operations allow safe concurrent access to shared data without explicit locking mechanisms like mutexes. The <code>sync/atomic</code> package provides low-level atomic memory primitives ideal for counters, flags, or simple state transitions.</p> <p>The key benefit of atomic operations is performance under contention. Locking introduces coordination overhead\u2014when many goroutines contend for a mutex, performance can degrade due to context switching and lock queue management. Atomics avoids this by operating directly at the hardware level using CPU instructions like <code>CAS</code> (compare-and-swap). This makes them particularly useful for:</p> <ul> <li>High-throughput counters and flags</li> <li>Lock-free queues and freelists</li> <li>Low-latency paths where locks are too expensive</li> </ul>"},{"location":"01-common-patterns/atomic-ops/#memory-model-and-comparison-to-c","title":"Memory Model and Comparison to C++","text":"<p>Understanding memory models is crucial when reasoning about concurrency. In C++, developers have fine-grained control over atomic operations via memory orderings, which allows them to trade-off between performance and consistency. By default, Go's atomic operations enforce sequential consistency, which means they behave like <code>std::memory_order_seq_cst</code> in C++. This is the strongest and safest memory ordering:</p> <ul> <li>All threads observe atomic operations in the same order.</li> <li>Full memory barrier are applied before and after each operation.</li> <li>Reads and writes are not reordered across atomic operations.</li> </ul> C++ Memory Order Go Equivalent Notes <code>memory_order_seq_cst</code> All <code>atomic.*</code> ops Full sequential consistency <code>memory_order_acquire</code> Not exposed Not available in Go <code>memory_order_release</code> Not exposed Not available in Go <code>memory_order_relaxed</code> Not exposed Not available in Go <p>Go does not expose weaker memory models like <code>relaxed</code>, <code>acquire</code>, or <code>release</code>. This is an intentional simplification to promote safety and reduce the risk of subtle data races. All atomic operations in Go imply synchronization across goroutines, ensuring correct behavior without manual memory fencing.</p> <p>This means you don\u2019t have to reason about instruction reordering or memory visibility at a low level\u2014but it also means you can\u2019t fine-tune for performance in the way C++ or Rust developers might use relaxed atomics.</p> <p>Low-level access to relaxed memory ordering in Go exists internally (e.g., in the runtime or through <code>go:linkname</code>), but it\u2019s not safe or supported for use in application-level code.</p>"},{"location":"01-common-patterns/atomic-ops/#common-atomic-operations","title":"Common Atomic Operations","text":"<ul> <li><code>atomic.AddInt64</code>, <code>atomic.AddUint32</code>, etc.: Adds values atomically.</li> <li><code>atomic.LoadInt64</code>, <code>atomic.LoadPointer</code>: Reads values atomically.</li> <li><code>atomic.StoreInt64</code>, <code>atomic.StorePointer</code>: Writes values atomically.</li> <li><code>atomic.CompareAndSwapInt64</code>: Conditionally updates a value atomically.</li> </ul>"},{"location":"01-common-patterns/atomic-ops/#when-to-use-atomic-operations-in-real-life","title":"When to Use Atomic Operations in Real Life","text":""},{"location":"01-common-patterns/atomic-ops/#high-throughput-metrics-and-counters","title":"High-throughput metrics and Counters","text":"<p>Tracking request counts, dropped packets, or other lightweight stats:</p> <pre><code>var requests atomic.Int64\n\nfunc handleRequest() {\n    requests.Add(1)\n}\n</code></pre> <p>This code allows multiple goroutines to safely increment a shared counter without using locks. <code>atomic.AddInt64</code> ensures each addition is performed atomically, preventing race conditions and keeping performance high under heavy load.</p>"},{"location":"01-common-patterns/atomic-ops/#fast-lock-free-flags","title":"Fast, Lock-Free Flags","text":"<p>Simple boolean state shared across threads:</p> <pre><code>var shutdown atomic.Int32\n\nfunc mainLoop() {\n    for {\n        if shutdown.Load() == 1 {\n            break\n        }\n        // do work\n    }\n}\n\nfunc stop() {\n    shutdown.Store(1)\n}\n</code></pre> <p>This pattern allows one goroutine to signal another to stop. <code>atomic.LoadInt32</code> reads the flag with synchronization guarantees, and <code>atomic.StoreInt32</code> sets the flag in a way visible to all goroutines. It helps implement safe shutdown signals.</p>"},{"location":"01-common-patterns/atomic-ops/#once-only-initialization","title":"Once-Only Initialization","text":"<p>Replace <code>sync.Once</code> when you need more control:</p> <pre><code>var initialized atomic.Int32\n\nfunc maybeInit() {\n    if initialized.CompareAndSwap(0, 1) {\n        // initialize resources\n    }\n}\n</code></pre> <p>This uses <code>CompareAndSwapInt32</code> to ensure only the first goroutine that sees <code>initialized == 0</code> will perform the initialization logic. All others skip it. It's efficient and avoids the lock overhead of <code>sync.Once</code>, especially when you need conditional or retryable behavior.</p>"},{"location":"01-common-patterns/atomic-ops/#lock-free-queues-or-freelist-structures","title":"Lock-Free Queues or Freelist Structures","text":"<p>Building high-performance data structures:</p> <pre><code>type node struct {\n    next *node\n    val  any\n}\n\nvar head atomic.Pointer[node]\n\nfunc push(n *node) {\n    for {\n        old := head.Load()\n        n.next = old\n        if head.CompareAndSwap(old, n) {\n            return\n        }\n    }\n}\n</code></pre> <p>This implements a lock-free stack (LIFO queue). It repeatedly tries to insert a node at the head of the list by atomically replacing the head pointer only if it hasn't changed\u2014a classic <code>CAS</code> loop. It's commonly used in object pools and work-stealing queues.</p>"},{"location":"01-common-patterns/atomic-ops/#reducing-lock-contention","title":"Reducing Lock Contention","text":"<p>This approach is common in real-world systems to reduce unnecessary lock contention, such as feature toggles, one-time initialization paths, or conditional caching mechanisms. Atomics serves as a fast-path filter before acquiring a more expensive lock.</p> <p>Combining atomics with mutexes to gate expensive work:</p> <pre><code>if atomic.LoadInt32(&amp;someFlag) == 0 {\n    return\n}\nmu.Lock()\ndefer mu.Unlock()\n// do something heavy\n</code></pre> <p>This pattern is effective when <code>someFlag</code> is set by another goroutine, and the current goroutine only uses it as a read-only signal to determine if it should proceed. It avoids unnecessary lock acquisition in high-throughput paths, such as short-circuiting when a feature is disabled or a task has already been completed.</p> <p>However, if the same goroutine is also responsible forsetting the flag, a simple load followed by a lock is not safe. Another goroutine could interleave between the check and the lock, leading to inconsistent behavior.</p> <p>To make the operation safe and atomic, use <code>CompareAndSwap</code>:</p> <pre><code>if !atomic.CompareAndSwapInt32(&amp;someFlag, 0, 1) {\n    return // work already in progress or completed\n}\nmu.Lock()\ndefer mu.Unlock()\n// perform one-time expensive initialization\n</code></pre> <p>This version guarantees that only one goroutine proceeds and others exit early. It ensures both the check and the update to <code>someFlag</code> happen atomically.</p> <p>Here, the atomic read acts as a fast gatekeeper. If the flag is unset, acquiring the mutex is unnecessary. This avoids unnecessary locking in high-frequency code paths, improving responsiveness under load.</p>"},{"location":"01-common-patterns/atomic-ops/#synchronization-primitives","title":"Synchronization Primitives","text":"<p>This section is intentionally kept minimal. Go's synchronization primitives\u2014such as <code>sync.Mutex</code>, <code>sync.RWMutex</code>, and <code>sync.Cond</code>\u2014are already thoroughly documented and widely understood. They are essential tools for managing shared memory and coordinating goroutines, but they are not the focus here.</p> <p>In the context of this article, we reference them only as a performance comparison baseline against atomic operations. When appropriate, these primitives offer clarity and correctness, but they often come at a higher cost in high-contention scenarios, where atomics can provide leaner alternatives.</p> <p>We\u2019ll use them as contrast points to highlight when and why atomic operations might offer performance advantages.</p>"},{"location":"01-common-patterns/atomic-ops/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>To understand the impact of atomic operations versus mutex locks, we can compare the time taken to increment a shared counter across goroutines using a simple benchmark.</p> <pre><code>func BenchmarkAtomicIncrement(b *testing.B) {\n    var counter int64\n    b.RunParallel(func(pb *testing.PB) {\n        for pb.Next() {\n            atomic.AddInt64(&amp;counter, 1)\n        }\n    })\n}\n\nfunc BenchmarkMutexIncrement(b *testing.B) {\n    var (\n        counter int64\n        mu      sync.Mutex\n    )\n    b.RunParallel(func(pb *testing.PB) {\n        for pb.Next() {\n            mu.Lock()\n            counter++\n            mu.Unlock()\n        }\n    })\n}\n</code></pre> <p>Benchmark results:</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkAtomicIncrement-14 39,910,514 80.40 0 0 BenchmarkMutexIncrement-14 32,629,298 110.7 0 0 <p>Atomic operations outperform mutex-based increments in both throughput and latency. The difference becomes more significant under higher contention, where avoiding lock acquisition helps reduce context switching and scheduler overhead.</p> Show the complete benchmark file <pre><code>package perf\n\nimport (\n    \"testing\"\n    \"sync/atomic\"\n    \"sync\"\n)\n\n// bench-start\nfunc BenchmarkAtomicIncrement(b *testing.B) {\n    var counter int64\n    b.RunParallel(func(pb *testing.PB) {\n        for pb.Next() {\n            atomic.AddInt64(&amp;counter, 1)\n        }\n    })\n}\n\nfunc BenchmarkMutexIncrement(b *testing.B) {\n    var (\n        counter int64\n        mu      sync.Mutex\n    )\n    b.RunParallel(func(pb *testing.PB) {\n        for pb.Next() {\n            mu.Lock()\n            counter++\n            mu.Unlock()\n        }\n    })\n}\n// bench-end\n</code></pre>"},{"location":"01-common-patterns/atomic-ops/#when-to-use-atomic-operations-vs-mutexes","title":"When to Use Atomic Operations vs. Mutexes","text":"<p> Atomic operations shine in simple, high-frequency scenarios\u2014counters, flags, coordination signals\u2014where the cost of a lock would be disproportionate. They avoid lock queues and reduce context switching. But they come with limitations: no grouping of multiple operations, no rollback, and increased complexity when applied beyond their niche.</p> <p> Mutexes remain the right tool for managing complex shared state, protecting multi-step critical sections, and maintaining invariants. They're easier to reason and generally safer when logic grows beyond a few lines.</p> <p>Choosing between atomics and locks isn't about ideology but scope. When the job is simple, atomics get out of the way. When the job gets complex, locks keep you safe.</p>"},{"location":"01-common-patterns/batching-ops/","title":"Batching Operations in Go","text":"<p>Batching is a simple but effective way to boost performance in high-throughput Go applications. By grouping multiple operations into a single call, you can minimize repeated overhead\u2014from network round-trips and disk I/O to database commits and CPU cycles. It\u2019s a practical technique that can make a big difference in both latency and resource usage.</p>"},{"location":"01-common-patterns/batching-ops/#why-batching-matters","title":"Why Batching Matters","text":"<p>Systems frequently encounter performance issues not because individual operations are inherently costly, but because they occur in high volume. Each call to external resources\u2014such as APIs, databases, or storage\u2014introduces latency, system calls, and potential context switching. Batching groups these operations to minimize repeated overhead, substantially improving throughput and efficiency.</p> <p>Consider a logging service writing to disk:</p> <pre><code>func logLine(line string) {\n    f.WriteString(line + \"\\n\")\n}\n</code></pre> <p>When invoked thousands of times per second, the file system is inundated with individual write system calls, significantly degrading performance. A better approach could be aggregates log entries and flushes them in bulk:</p> <pre><code>var batch []string\n\nfunc logBatch(line string) {\n    batch = append(batch, line)\n    if len(batch) &gt;= 100 {\n        f.WriteString(strings.Join(batch, \"\\n\") + \"\\n\")\n        batch = batch[:0]\n    }\n}\n</code></pre> <p>With batching, each write operation handles multiple entries simultaneously, reducing syscall overhead and improving disk I/O efficiency.</p> <p>Warning</p> <p>While batching offers substantial performance advantages, it also introduces the risk of data loss. If an application crashes before a batch is flushed, the in-memory data can be lost. Systems dealing with critical or transactional data must incorporate safeguards such as periodic flushes, persistent storage buffers, or recovery mechanisms to mitigate this risk.</p>"},{"location":"01-common-patterns/batching-ops/#how-generic-batcher-may-looks-like","title":"How generic Batcher may looks like","text":"<p>We can implement a generic batcher in very straight forward manner:</p> <pre><code>type Batcher[T any] struct {\n    mu     sync.Mutex\n    buffer []T\n    size   int\n    flush  func([]T)\n}\n\nfunc NewBatcher[T any](size int, flush func([]T)) *Batcher[T] {\n    return &amp;Batcher[T]{\n        buffer: make([]T, 0, size),\n        size:   size,\n        flush:  flush,\n    }\n}\n\nfunc (b *Batcher[T]) Add(item T) {\n    b.mu.Lock()\n    defer b.mu.Unlock()\n    b.buffer = append(b.buffer, item)\n    if len(b.buffer) &gt;= b.size {\n        b.flushNow()\n    }\n}\n\nfunc (b *Batcher[T]) flushNow() {\n    if len(b.buffer) == 0 {\n        return\n    }\n    b.flush(b.buffer)\n    b.buffer = b.buffer[:0]\n}\n</code></pre> <p>Warning</p> <p>This batcher implementation expects that you will never call <code>Batcher.Add(...)</code> from your <code>flush()</code> function. We have this limitation because Go mutexes are not recursive.</p> <p>This batcher works with any data type, making it a flexible solution for aggregating logs, metrics, database writes, or other grouped operations. Internally, the buffer acts as a queue that accumulates items until a flush threshold is reached. The use of <code>sync.Mutex</code> ensures that <code>Add()</code> and <code>flushNow()</code> are safe for concurrent access, which is necessary in most real-world systems where multiple goroutines may write to the batcher.</p> <p>From a performance standpoint, it's true that a lock-free implementation\u2014using atomic operations or concurrent ring buffers\u2014could reduce contention and improve throughput under heavy load. However, such designs are more complex, harder to maintain, and generally not justified unless you're pushing extremely high concurrency or low-latency boundaries. For most practical workloads, the simplicity and safety of a <code>sync.Mutex</code>-based design offers a great balance between performance and maintainability.</p>"},{"location":"01-common-patterns/batching-ops/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>To validate batching performance, we tested six scenarios across three categories: in-memory processing, file I/O, and CPU-intensive hashing. Each category included both unbatched and batched variants, with all benchmarks running over 10,000 items per operation.</p> Show the benchmark file <pre><code>package perf\n\nimport (\n    \"crypto/sha256\"\n    \"encoding/hex\"\n    \"fmt\"\n    \"os\"\n    \"strings\"\n    \"testing\"\n)\n\nvar lines = make([]string, 10000)\n\nfunc init() {\n    for i := range lines {\n        lines[i] = fmt.Sprintf(\"log entry %d %s\", i, strings.Repeat(\"x\", 100))\n    }\n}\n\n// --- 1. No I/O ---\n\nfunc BenchmarkUnbatchedProcessing(b *testing.B) {\n    for b.Loop() {\n        for _, line := range lines {\n            strings.ToUpper(line)\n        }\n    }\n}\n\nfunc BenchmarkBatchedProcessing(b *testing.B) {\n    batchSize := 100\n    for b.Loop() {\n        for i := 0; i &lt; len(lines); i += batchSize {\n            end := i + batchSize\n            if end &gt; len(lines) {\n                end = len(lines)\n            }\n            batch := strings.Join(lines[i:end], \"|\")\n            strings.ToUpper(batch)\n        }\n    }\n}\n\n// --- 2. With I/O ---\n\nfunc BenchmarkUnbatchedIO(b *testing.B) {\n    for b.Loop() {\n        f, err := os.CreateTemp(\"\", \"unbatched\")\n        if err != nil {\n            b.Fatal(err)\n        }\n        for _, line := range lines {\n            _, _ = f.WriteString(line + \"\\n\")\n        }\n        f.Close()\n        os.Remove(f.Name())\n    }\n}\n\nfunc BenchmarkBatchedIO(b *testing.B) {\n    batchSize := 100\n    for b.Loop() {\n        f, err := os.CreateTemp(\"\", \"batched\")\n        if err != nil {\n            b.Fatal(err)\n        }\n        for i := 0; i &lt; len(lines); i += batchSize {\n            end := i + batchSize\n            if end &gt; len(lines) {\n                end = len(lines)\n            }\n            batch := strings.Join(lines[i:end], \"\\n\") + \"\\n\"\n            _, _ = f.WriteString(batch)\n        }\n        f.Close()\n        os.Remove(f.Name())\n    }\n}\n\n// --- 3. With Crypto ---\n\nfunc hash(s string) string {\n    h := sha256.Sum256([]byte(s))\n    return hex.EncodeToString(h[:])\n}\n\nfunc BenchmarkUnbatchedCrypto(b *testing.B) {\n    for b.Loop() {\n        for _, line := range lines {\n            hash(line)\n        }\n    }\n}\n\nfunc BenchmarkBatchedCrypto(b *testing.B) {\n    batchSize := 100\n    for b.Loop() {\n        for i := 0; i &lt; len(lines); i += batchSize {\n            end := i + batchSize\n            if end &gt; len(lines) {\n                end = len(lines)\n            }\n            joined := strings.Join(lines[i:end], \"\")\n            hash(joined)\n        }\n    }\n}\n</code></pre> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkUnbatchedProcessing-14 530 2,028,492 1,279,850 10,000 BenchmarkBatchedProcessing-14 573 2,094,168 2,457,603 200 <p>In-memory string manipulation showed a modest performance delta. While the batched variant reduced memory allocations by 50x, the execution time was only marginally slower due to the cost of joining large strings. This highlights that batching isn\u2019t always faster in raw throughput, but it consistently reduces pressure on the garbage collector.</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkUnbatchedIO-14 87 12,766,433 1,280,424 10,007 BenchmarkBatchedIO-14 1324 993,912 2,458,026 207 <p>File I/O benchmarks showed the most dramatic gains. The batched version was over 12 times faster than the unbatched one, with far fewer syscalls and significantly lower execution time. Grouping disk writes amortized the I/O cost, leading to a huge efficiency boost despite temporarily using more memory.</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkUnbatchedCrypto-14 978 1,232,242 2,559,840 30,000 BenchmarkBatchedCrypto-14 1760 675,303 2,470,406 400 <p>The cryptographic benchmarks demonstrated batching\u2019s value in CPU-bound scenarios. Batched hashing nearly halved the total processing time while reducing allocation count by more than 70x. This reinforces batching as an effective strategy even in CPU-intensive workloads where fewer operations yield better locality and cache behavior.</p>"},{"location":"01-common-patterns/batching-ops/#when-to-use-batching","title":"When To Use Batching","text":"<p> Use batching when:</p> <ul> <li>Individual operations are expensive (e.g., I/O, RPC, DB writes). Grouping multiple operations into a single batch reduces the overhead of repeated calls and improves efficiency.</li> <li>The system benefits from reducing the frequency of external interactions. Fewer external calls can ease load on downstream systems and reduce contention or rate-limiting issues.</li> <li>You have some tolerance for per-item latency in favor of higher throughput. Batching introduces slight delays but can significantly increase overall system throughput.</li> </ul> <p> Avoid batching when:</p> <ul> <li>Immediate action is required for each individual input. Delaying processing to build a batch may violate time-sensitive requirements.</li> <li>Holding data introduces risk (e.g., crash before flush). If data must be processed or persisted immediately to avoid loss, batching can be unsafe.</li> <li>Predictable latency is more important than throughput. Batching adds variability in timing, which may not be acceptable in systems with strict latency expectations.</li> </ul>"},{"location":"01-common-patterns/buffered-io/","title":"Efficient Buffering in Go","text":"<p>Buffering is a core performance technique in systems programming. In Go, it's especially relevant when working with I/O\u2014file access, network communication, and stream processing. Without buffering, many operations incur excessive system calls or synchronization overhead. Proper buffering reduces the frequency of such interactions, improves throughput, and smooths latency spikes.</p>"},{"location":"01-common-patterns/buffered-io/#why-buffering-matters","title":"Why Buffering Matters","text":"<p>Every read or write to a file or socket may invoke a syscall or context switch. System calls involve a costly transition from user space (where your application runs) into kernel space (managed by the operating system). This overhead includes kernel mode entry, potential context switching, disk buffering, and queuing of write operations. By aggregating data in memory and writing or reading in chunks, buffered operations dramatically reduce the frequency and cost of these syscalls.</p> <p>For example, writing to a file in a loop without buffering, like this:</p> <pre><code>f, _ := os.Create(\"output.txt\")\nfor i := 0; i &lt; 10000; i++ {\n    f.Write([]byte(\"line\\n\"))\n}\n</code></pre> <p>results in 10,000 separate system calls, greatly increasing overhead and slowing down your application. Additionally, repeated small writes fragment disk operations, placing unnecessary strain on system resources.</p>"},{"location":"01-common-patterns/buffered-io/#with-buffering","title":"With Buffering","text":"<pre><code>f, _ := os.Create(\"output.txt\")\nbuf := bufio.NewWriter(f)\nfor i := 0; i &lt; 10000; i++ {\n    buf.WriteString(\"line\\n\")\n}\nbuf.Flush() // ensure all buffered data is written\n</code></pre> <p>This version significantly reduces the number of system calls. The <code>bufio.Writer</code> accumulates writes in an internal memory buffer (typically 4KB or more). It only triggers a syscall when the buffer is full or explicitly flushed. As a result, you achieve faster I/O, reduced CPU usage, and improved performance.</p> <p>Note</p> <p><code>bufio.Writer</code> does not automatically flush when closed. If you forget to call <code>Flush()</code>, any unwritten data remaining in the buffer will be lost. Always call <code>Flush()</code> before closing or returning from a function, especially if the total written size is smaller than the buffer capacity.</p>"},{"location":"01-common-patterns/buffered-io/#controlling-buffer-capacity","title":"Controlling Buffer Capacity","text":"<p>By default, <code>bufio.NewWriter()</code> allocates a 4096-byte (4 KB) buffer. This size aligns with the common block size of file systems and the standard memory page size on most operating systems (such as Linux, BSD, and macOS). Reading or writing in 4 KB increments minimizes page faults, aligns with kernel read-ahead strategies, and maps efficiently onto underlying disk I/O operations.</p> <p>While 4 KB is a practical general-purpose default, it might not be optimal for all workloads. For high-throughput scenarios\u2014such as streaming large files or generating extensive logs\u2014a larger buffer can help reduce syscall frequency further:</p> <pre><code>f, _ := os.Create(\"output.txt\")\nbuf := bufio.NewWriterSize(f, 16*1024) // 16 KB buffer\n</code></pre> <p>Conversely, if latency is more critical than throughput (e.g., interactive systems or command-line utilities), a smaller buffer may be more appropriate, as it flushes data more frequently.</p> <p>Similar logic applies when reading data:</p> <pre><code>reader := bufio.NewReaderSize(f, 32*1024) // 32 KB buffer for input\n</code></pre> <p>Choosing buffer sizes should always be guided by profiling and empirical performance testing. Factors like disk type (SSD vs. HDD), file system configurations, CPU cache sizes, and overall system load can influence optimal buffer sizing.</p>"},{"location":"01-common-patterns/buffered-io/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>Buffered writes and reads consistently demonstrate significant performance gains under load. Benchmarks measuring system calls, memory allocations, and CPU usage typically show that buffered I/O operations are faster and more efficient than unbuffered counterparts. For example, writing one million lines to disk might exhibit up to an order-of-magnitude improvement using <code>bufio.Writer</code> compared to direct <code>os.File.Write()</code> calls. The more structured and bursty your I/O operations, the more substantial the benefits from buffering.</p> Show the benchmark file <pre><code>package perf\n\nimport (\n    \"bufio\"\n    \"io\"\n    \"os\"\n    \"strconv\"\n    \"sync\"\n    \"testing\"\n)\n\ntype Data struct {\n    Value []byte\n}\n\nvar dataPool = sync.Pool{\n    New: func() any {\n        return &amp;Data{Value: make([]byte, 0, 32)}\n    },\n}\n\nconst N = 10000\n\nfunc writeNotBuffered(w io.Writer, count int) {\n    for i := 0; i &lt; count; i++ {\n        d := dataPool.Get().(*Data)\n        d.Value = strconv.AppendInt(d.Value[:0], int64(i), 10)\n        w.Write(d.Value)\n        w.Write([]byte(\":val\\n\"))\n        dataPool.Put(d)\n    }\n}\n\nfunc writeBuffered(w io.Writer, count int) {\n    buf := bufio.NewWriterSize(w, 16*1024)\n    for i := 0; i &lt; count; i++ {\n        d := dataPool.Get().(*Data)\n        d.Value = strconv.AppendInt(d.Value[:0], int64(i), 10)\n        buf.Write(d.Value)\n        buf.Write([]byte(\":val\\n\"))\n        dataPool.Put(d)\n    }\n    buf.Flush()\n}\n\nfunc BenchmarkWriteNotBuffered(b *testing.B) {\n    for b.Loop() {\n        f, _ := os.CreateTemp(\"\", \"nobuf\")\n        writeNotBuffered(f, N)\n        f.Close()\n        os.Remove(f.Name())\n    }\n}\n\nfunc BenchmarkWriteBuffered(b *testing.B) {\n    for b.Loop() {\n        f, _ := os.CreateTemp(\"\", \"buf\")\n        writeBuffered(f, N)\n        f.Close()\n        os.Remove(f.Name())\n    }\n}\n</code></pre> <p>Results:</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkWriteNotBuffered-14 49 23,672,792 53,773 10,007 BenchmarkWriteBuffered-14 3241 379,703 70,127 10,008"},{"location":"01-common-patterns/buffered-io/#when-to-buffer","title":"When To Buffer","text":"<p> Use buffering when:</p> <ul> <li>Performing frequent, small-sized I/O operations. Buffering groups small writes or reads into larger batches, which reduces the overhead of each individual operation.</li> <li>Reducing syscall overhead is crucial. Fewer syscalls mean lower context-switching costs and improved performance, especially in I/O-heavy applications.</li> <li>High throughput is more important than minimal latency. Buffered I/O can increase total data processed per second, even if it introduces slight delays in delivery.</li> </ul> <p> Avoid buffering when:</p> <ul> <li>Immediate data availability and low latency are critical. Buffers introduce delays by design, which can be unacceptable in real-time or interactive systems.</li> <li>Buffering excessively might lead to uncontrolled memory usage. Without limits or proper flushing, buffers can grow large and put pressure on system memory.</li> </ul>"},{"location":"01-common-patterns/comp-flags/","title":"Leveraging Compiler Optimization Flags in Go","text":"<p>When optimizing Go applications for performance, we often focus on profiling, memory allocations, or concurrency patterns. But another layer worth considering is how the Go compiler optimizes your code during the build process.</p> <p>While Go doesn\u2019t expose the same granular set of compiler flags as C or Rust, it still provides useful ways to influence how your code is built\u2014especially when targeting performance, binary size, or specific environments.</p>"},{"location":"01-common-patterns/comp-flags/#why-compiler-flags-matter","title":"Why Compiler Flags Matter","text":"<p>Go's compiler (specifically <code>cmd/compile</code> and <code>cmd/link</code>) performs several default optimizations: inlining, escape analysis, dead code elimination, and more. However, there are scenarios where you can squeeze more performance or control from your build using the right flags.</p> <p>Use cases include:</p> <ul> <li>Reducing binary size for minimal containers or embedded systems  </li> <li>Building for specific architectures or OSes  </li> <li>Removing debug information for release builds  </li> <li>Disabling optimizations temporarily for easier debugging  </li> <li>Enabling experimental or unsafe performance tricks (carefully)</li> </ul>"},{"location":"01-common-patterns/comp-flags/#key-compiler-and-linker-flags","title":"Key Compiler and Linker Flags","text":""},{"location":"01-common-patterns/comp-flags/#-ldflags-s-w-strip-debug-info","title":"<code>-ldflags=\"-s -w\"</code> \u2014 Strip Debug Info","text":"<p>When you want to shrink binary size, especially in production or containers:</p> <pre><code>go build -ldflags=\"-s -w\" -o app main.go\n</code></pre> <ul> <li><code>-s</code>: Omit the symbol table</li> <li><code>-w</code>: Omit DWARF debugging information</li> </ul> <p>Why it matters: This can reduce binary size by up to 30-40%, depending on your codebase. It is useful in Docker images or when distributing binaries.</p>"},{"location":"01-common-patterns/comp-flags/#-gcflags-control-compiler-optimizations","title":"<code>-gcflags</code> \u2014 Control Compiler Optimizations","text":"<p>The <code>-gcflags</code> flag allows you to control how the compiler treats specific packages. For example, you cab disable optimizations for debugging:</p> <pre><code>go build -gcflags=\"all=-N -l\" -o app main.go\n</code></pre> <ul> <li><code>-N</code>: Disable optimizations</li> <li><code>-l</code>: Disable inlining</li> </ul> <p>When to use: During debugging sessions with Delve or similar tools. Turning off inlining and optimizations make stack traces and breakpoints more reliable.</p>"},{"location":"01-common-patterns/comp-flags/#cross-compilation-flags","title":"Cross-Compilation Flags","text":"<p>Need to build for another OS or architecture?</p> <pre><code>GOOS=linux GOARCH=arm64 go build -o app main.go\n</code></pre> <ul> <li><code>GOOS</code>, <code>GOARCH</code>: Set target OS and architecture</li> <li>Common values: <code>windows</code>, <code>darwin</code>, <code>linux</code>, <code>amd64</code>, <code>arm64</code>, <code>386</code>, <code>wasm</code></li> </ul>"},{"location":"01-common-patterns/comp-flags/#build-tags","title":"Build Tags","text":"<p>Build tags allow conditional compilation. Use <code>//go:build</code> or <code>// +build</code> in your source code to control what gets compiled in.</p> <p>Example:</p> <pre><code>//go:build debug\n\npackage main\n\nimport \"log\"\n\nfunc debugLog(msg string) {\n    log.Println(\"[DEBUG]\", msg)\n}\n</code></pre> <p>Then build with:</p> <pre><code>go build -tags=debug -o app main.go\n</code></pre>"},{"location":"01-common-patterns/comp-flags/#-ldflags-x-inject-build-time-variables","title":"<code>-ldflags=\"-X ...\"</code> \u2014 Inject Build-Time Variables","text":"<p>You can inject version numbers or metadata into your binary at build time:</p> <pre><code>// main.go\npackage main\n\nimport \"fmt\"\n\nvar version = \"dev\"\n\nfunc main() {\n    fmt.Printf(\"App version: %s\\n\", version)\n}\n</code></pre> <p>Then build with:</p> <pre><code>go build -ldflags=\"-s -w -X main.version=1.0.0\" -o app main.go\n</code></pre> <p>This sets the <code>version</code> variable at link time without modifying your source code. It's useful for embedding release versions, commit hashes, or build dates.</p>"},{"location":"01-common-patterns/comp-flags/#-extldflags-static-build-fully-static-binaries","title":"<code>-extldflags='-static'</code> \u2014 Build Fully Static Binaries","text":"<p>The <code>-extldflags '-static'</code> option passes the <code>-static</code> flag to the external system linker, instructing it to produce a fully statically linked binary.</p> <p>This is especially useful when you're using CGO and want to avoid runtime dynamic library dependencies:</p> <pre><code>CGO_ENABLED=1 GOOS=linux GOARCH=amd64 \\\nCC=gcc \\\ngo build -ldflags=\"-linkmode=external -extldflags '-static'\" -o app main.go\n</code></pre> <p>What it does:</p> <ul> <li>Statically links all C libraries into the binary</li> <li>Produces a portable, self-contained executable</li> <li>Ideal for minimal containers (like <code>scratch</code> or <code>distroless</code>)</li> </ul> <p>To go further and ensure your binary avoids relying on C library DNS resolution (such as <code>glibc</code>'s <code>getaddrinfo</code>), you can use the <code>netgo</code> build tag. This forces Go to use its pure Go implementation of the DNS resolver:</p> <pre><code>CGO_ENABLED=1 GOOS=linux GOARCH=amd64 \\\nCC=gcc \\\ngo build -tags netgo -ldflags=\"-linkmode=external -extldflags '-static'\" -o app main.go\n</code></pre> <p>This step is especially important when building for minimal container environments, where dynamic libc dependencies may not be available.</p> <p>Note</p> <p>Static linking requires static versions (<code>.a</code>) of the libraries you're using, and may not work with all C libraries by default.</p>"},{"location":"01-common-patterns/comp-flags/#example-static-build-with-libcurl-via-cgo","title":"Example: Static Build with libcurl via CGO","text":"<p>If you\u2019re using libcurl via CGO, here\u2019s how you can create a statically linked Go binary:</p> <pre><code>package main\n\n/*\n#cgo LDFLAGS: -lcurl\n#include &lt;curl/curl.h&gt;\n*/\nimport \"C\"\nimport \"fmt\"\n\nfunc main() {\n    fmt.Println(\"libcurl version:\", C.GoString(C.curl_version()))\n}\n</code></pre> <p>Static Build Command:</p> <pre><code>CGO_ENABLED=1 GOOS=linux GOARCH=amd64 \\\nCC=gcc \\\ngo build -tags netgo -ldflags=\"-linkmode=external -extldflags '-static'\" -o app main.go\n</code></pre> <p>Ensure the static version of libcurl (<code>libcurl.a</code>) is available on your system. You may need to install development packages or build libcurl from source with <code>--enable-static</code>.</p>"},{"location":"01-common-patterns/context/","title":"Efficient Context Management","text":"<p>Whether you're handling HTTP requests, coordinating worker goroutines, or querying external services, there's often a need to cancel in-flight operations or enforce execution deadlines. Go\u2019s <code>context</code> package is designed for precisely that\u2014it provides a consistent and thread-safe way to manage operation lifecycles, propagate metadata, and ensure resources are cleaned up promptly.</p>"},{"location":"01-common-patterns/context/#why-context-matters","title":"Why Context Matters","text":"<p>Go provides two base context constructors: <code>context.Background()</code> and <code>context.TODO()</code>.</p> <ul> <li><code>context.Background()</code> is the root context typically used at the top level of your application\u2014such as in <code>main</code>, <code>init</code>, or server setup\u2014where no existing context is available.</li> <li><code>context.TODO()</code> is a placeholder used when it\u2019s unclear which context to use, or when the surrounding code hasn\u2019t yet been fully wired for context propagation. It serves as a reminder that the context logic needs to be filled in later.</li> </ul> <p>The <code>context</code> package in Go is designed to carry deadlines, cancellation signals, and other request-scoped values across API boundaries. It's especially useful in concurrent programs where operations need to be coordinated and canceled cleanly.</p> <p>A typical context workflow begins at the entry point of a program or request\u2014like an HTTP handler, main function, or RPC server. From there, a base context is created using <code>context.Background()</code> or <code>context.TODO()</code>. This context can then be extended using constructors like:</p> <ul> <li><code>context.WithCancel(parent)</code> to create a cancelable context.</li> <li><code>context.WithTimeout(parent, duration)</code> to cancel automatically after a specific time.</li> <li><code>context.WithDeadline(parent, time)</code> for cancelling at a fixed moment.</li> <li><code>context.WithValue(parent, key, value)</code> to attach request-scoped data.</li> </ul> <p>Each of these functions returns a new context that wraps its parent. Cancellation signals, deadlines, and values are automatically propagated down the call stack. When a context is canceled\u2014either manually or by timeout\u2014any goroutines or functions listening on <code>&lt;-ctx.Done()</code> are immediately notified.</p> <p>By passing context explicitly through function parameters, you avoid hidden dependencies and gain fine-grained control over the execution lifecycle of concurrent operations.</p>"},{"location":"01-common-patterns/context/#practical-examples-of-context-usage","title":"Practical Examples of Context Usage","text":"<p>The following examples show how <code>context.Context</code> enables better control, observability, and resource management across a variety of real-world scenarios.</p>"},{"location":"01-common-patterns/context/#http-server-request-cancellation","title":"HTTP Server Request Cancellation","text":"<p>Contexts help gracefully handle cancellations when clients disconnect early. Every incoming HTTP request in Go carries a context that gets canceled if the client closes the connection. By checking <code>&lt;-ctx.Done()</code>, you can exit early instead of doing unnecessary work:</p> <pre><code>func handler(w http.ResponseWriter, req *http.Request) {\n    ctx := req.Context()\n    select {\n    case &lt;-time.After(5 * time.Second):\n        fmt.Fprintln(w, \"Response after delay\")\n    case &lt;-ctx.Done():\n        log.Println(\"Client disconnected\")\n    }\n}\n</code></pre> <p>In this example, the handler waits for either a simulated delay or cancellation. If the client closes the connection before the timeout, <code>ctx.Done()</code> is triggered, allowing the handler to clean up without writing a response.</p>"},{"location":"01-common-patterns/context/#database-operations-with-timeouts","title":"Database Operations with Timeouts","text":"<p>Contexts provide a straightforward way to enforce timeouts on database queries. Many drivers support <code>QueryContext</code> or similar methods that respect cancellation:</p> <pre><code>ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)\ndefer cancel()\n\nrows, err := db.QueryContext(ctx, \"SELECT * FROM users\")\nif err != nil {\n    log.Fatal(err)\n}\ndefer rows.Close()\n</code></pre> <p>In this case, the context is automatically canceled if the database does not respond within two seconds. The query is aborted, and the application doesn\u2019t hang indefinitely. This helps manage resources and avoids cascading failures in high-load environments.</p>"},{"location":"01-common-patterns/context/#propagating-request-ids-for-distributed-tracing","title":"Propagating Request IDs for Distributed Tracing","text":"<p>Contexts allow passing tracing information across different layers of a distributed system. For example, a request ID generated at the edge can be attached to the context and logged or used throughout the application:</p> <pre><code>func main() {\n    ctx := context.WithValue(context.Background(), \"requestID\", \"12345\")\n    handleRequest(ctx)\n}\n\nfunc handleRequest(ctx context.Context) {\n    log.Printf(\"Handling request with ID: %v\", ctx.Value(\"requestID\"))\n}\n</code></pre> <p>In this example, <code>WithValue</code> attaches a request ID to the context. The function <code>handleRequest</code> retrieves it using <code>ctx.Value</code>, enabling consistent logging and observability without modifying function signatures. This approach is common in middleware, logging, and tracing pipelines.</p>"},{"location":"01-common-patterns/context/#concurrent-worker-management","title":"Concurrent Worker Management","text":"<p>Context provides control over multiple worker goroutines. By using <code>WithCancel</code>, you can propagate a stop signal to all workers from a central point:</p> <pre><code>ctx, cancel := context.WithCancel(context.Background())\n\nfor i := 0; i &lt; 10; i++ {\n    go worker(ctx, i)\n}\n\n// Cancel workers after some condition or signal\ncancel()\n</code></pre> <p>Each worker function should check for <code>&lt;-ctx.Done()</code> and return immediately when the context is canceled. This keeps the system responsive, avoids dangling goroutines, and allows graceful termination of parallel work.</p>"},{"location":"01-common-patterns/context/#graceful-shutdown-in-cli-tools","title":"Graceful Shutdown in CLI Tools","text":"<p>In command-line applications or long-running background processes, context simplifies OS signal handling and graceful shutdown:</p> <pre><code>ctx, stop := signal.NotifyContext(context.Background(), os.Interrupt)\ndefer stop()\n\n&lt;-ctx.Done()\nfmt.Println(\"Shutting down...\")\n</code></pre> <p>In this pattern, <code>signal.NotifyContext</code> returns a context that is canceled automatically when an interrupt signal (e.g., Ctrl+C) is received. Listening on <code>&lt;-ctx.Done()</code> allows the application to perform cleanup and exit gracefully instead of terminating abruptly.</p>"},{"location":"01-common-patterns/context/#streaming-and-real-time-data-pipelines","title":"Streaming and Real-Time Data Pipelines","text":"<p>Context is ideal for coordinating readers in streaming systems like Kafka consumers, WebSocket readers, or custom pub/sub pipelines:</p> <pre><code>func streamData(ctx context.Context, ch &lt;-chan Data) {\n    for {\n        select {\n        case &lt;-ctx.Done():\n            return\n        case data := &lt;-ch:\n            process(data)\n        }\n    }\n}\n</code></pre> <p>Here, the function processes incoming data from a channel. If the context is canceled (e.g., during shutdown or timeout), the loop breaks and the goroutine exits cleanly. This makes the system more responsive to control signals and easier to manage under load.</p>"},{"location":"01-common-patterns/context/#middleware-and-rate-limiting","title":"Middleware and Rate Limiting","text":"<p>Contexts are often used in middleware chains to enforce quotas, trace requests, or carry rate-limit decisions between layers. In a typical HTTP stack, middleware can determine whether a request is allowed based on custom logic (e.g., IP-based rate limiting or user quota checks), and attach that decision to the context so that downstream handlers can inspect it.</p> <p>Here's a simplified example of how that might work:</p> <pre><code>func rateLimitMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        // Suppose this is the result of some rate-limiting logic\n        rateLimited := true // or false depending on logic\n\n        // Embed the result into the context\n        ctx := context.WithValue(r.Context(), \"rateLimited\", rateLimited)\n\n        // Pass the updated context to the next handler\n        next.ServeHTTP(w, r.WithContext(ctx))\n    })\n}\n</code></pre> <p>In a downstream handler, you might inspect that value like so:</p> <pre><code>func handler(w http.ResponseWriter, r *http.Request) {\n    ctx := r.Context()\n    if limited, ok := ctx.Value(\"rateLimited\").(bool); ok &amp;&amp; limited {\n        http.Error(w, \"Too many requests\", http.StatusTooManyRequests)\n        return\n    }\n    fmt.Fprintln(w, \"Request accepted\")\n}\n</code></pre> <p>This pattern avoids the need for shared state between middleware and handlers. Instead, the context acts as a lightweight channel for passing metadata between layers of the request pipeline in a safe and composable way.</p>"},{"location":"01-common-patterns/context/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>There's usually nothing to benchmark directly in terms of raw performance when using <code>context.Context</code>. Its real benefit lies in improving responsiveness, avoiding wasted computation, and enabling clean cancellations. The impact shows up in reduced memory leaks, fewer stuck goroutines, and more predictable resource lifetimes\u2014metrics best observed through real-world profiling and observability tools.</p>"},{"location":"01-common-patterns/context/#best-practices-for-context-usage","title":"Best Practices for Context Usage","text":"<ul> <li>Always pass <code>context.Context</code> explicitly, typically as the first argument to a function. This makes context propagation transparent and traceable, especially across API boundaries or service layers. Don\u2019t store contexts in struct fields or global variables. Doing so can lead to stale contexts being reused unintentionally and make cancellation logic harder to reason about.</li> <li>Use 1 only for request-scoped metadata, not to pass business logic or application state. Overusing context for general-purpose data storage leads to tight coupling and makes testing and tracing harder.</li> <li>Check <code>ctx.Err()</code> to differentiate between <code>context.Canceled</code> and <code>context.DeadlineExceeded</code> where needed. This allows your application to respond appropriately\u2014for example, distinguishing between user-initiated cancellation and timeouts.</li> </ul> <p>Following these practices helps keep context usage predictable and idiomatic.</p>"},{"location":"01-common-patterns/fields-alignment/","title":"Struct Field Alignment","text":"<p>When optimizing Go programs for performance, struct layout and memory alignment often go unnoticed\u2014yet they have a measurable impact on memory usage and cache efficiency. Go automatically aligns struct fields based on platform-specific rules, inserting padding to satisfy alignment constraints. Understanding and controlling this alignment can reduce memory footprint, improve cache locality, and improve performance in tight loops or high-throughput data pipelines.</p>"},{"location":"01-common-patterns/fields-alignment/#why-alignment-matters","title":"Why Alignment Matters","text":"<p>Modern CPUs are sensitive to memory layout. When data is misaligned or spans multiple cache lines, it incurs additional access cycles and can disrupt performance. In Go, struct fields are aligned according to their type requirements, and the compiler inserts padding bytes to meet these constraints. If fields are arranged without care, unnecessary padding may inflate struct size significantly, affecting memory use and bandwidth.</p> <p>Consider the following two structs:</p> <pre><code>type PoorlyAligned struct {\n    flag bool\n    count int64\n    id byte\n}\n\ntype WellAligned struct {\n    count int64\n    flag bool\n    id byte\n}\n</code></pre> <p>On a 64-bit system, <code>PoorlyAligned</code> requires 24 bytes due to the padding between fields, whereas <code>WellAligned</code> fits into 16 bytes by ordering fields from largest to smallest alignment requirement.</p>"},{"location":"01-common-patterns/fields-alignment/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>We benchmarked both struct layouts by allocating 10 million instances of each and measuring allocation time and memory usage:</p> <pre><code>func BenchmarkPoorlyAligned(b *testing.B) {\n    for b.Loop() {\n        var items = make([]PoorlyAligned, 10_000_000)\n        for j := range items {\n            items[j].count = int64(j)\n        }\n    }\n}\n\nfunc BenchmarkWellAligned(b *testing.B) {\n    for b.Loop() {\n        var items = make([]WellAligned, 10_000_000)\n        for j := range items {\n            items[j].count = int64(j)\n        }\n    }\n}\n</code></pre> <p>Benchmark Results</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op PoorlyAligned-14 177 20,095,621 240,001,029 1 WellAligned-14 186 19,265,714 160,006,148 1 <p>The WellAligned version reduced memory usage by 80MB for 10 million structs and also ran slightly faster than the poorly aligned version. This highlights that thoughtful field arrangement improves memory efficiency and can yield modest performance gains in allocation-heavy code paths.</p>"},{"location":"01-common-patterns/fields-alignment/#avoiding-false-sharing-in-concurrent-workloads","title":"Avoiding False Sharing in Concurrent Workloads","text":"<p>In addition to memory layout efficiency, struct alignment also plays a crucial role in concurrent systems. When multiple goroutines access different fields of the same struct that reside on the same CPU cache line, they may suffer from false sharing\u2014where changes to one field cause invalidations in the other, even if logically unrelated.</p> <p>On modern CPUs, a typical cache line is 64 bytes wide. When a struct is accessed in memory, the CPU loads the entire cache line that contains it, not just the specific field. This means that two unrelated fields within the same 64-byte block will both reside in the same line\u2014even if they are used independently by separate goroutines. If one goroutine writes to its field, the cache line becomes invalidated and must be reloaded on the other core, leading to degraded performance due to false sharing.</p> <p>To illustrate, we compared two structs\u2014one vulnerable to false sharing, and another with padding to separate fields across cache lines:</p> <pre><code>type SharedCounterBad struct {\n    a int64\n    b int64\n}\n\ntype SharedCounterGood struct {\n    a int64\n    _ [56]byte // Padding to prevent a and b from sharing a cache line\n    b int64\n}\n</code></pre> <p>Each field is incremented by a separate goroutine 1 million times:</p> <pre><code>func BenchmarkFalseSharing(b *testing.B) {\n    var c SharedCounterBad  // (1)\n    var wg sync.WaitGroup\n\n    for b.Loop() {\n        wg.Add(2)\n        go func() {\n            for i := 0; i &lt; 1_000_000; i++ {\n                c.a++\n            }\n            wg.Done()\n        }()\n        go func() {\n            for i := 0; i &lt; 1_000_000; i++ {\n                c.b++\n            }\n            wg.Done()\n        }()\n        wg.Wait()\n    }\n}\n</code></pre> <ol> <li><code>FalseSharing</code> and <code>NoFalseSharing</code> benchmarks are identical, except we will use <code>SharedCounterGood</code> for the <code>NoFalseSharing</code> benchmark.</li> </ol> <p>Benchmark Results:</p> Benchmark Time per op (ns) Bytes per op Allocs per op FalseSharing 996,234 55 2 NoFalseSharing 958,180 58 2 <p>Placing padding between the two fields prevented false sharing, resulting in a measurable performance improvement. The version with padding completed ~3.8% faster (the value could vary between re-runs from 3% to 6%), which can make a difference in tight concurrent loops or high-frequency counters. It also shows how false sharing may unpredictably affect memory use due to invalidation overhead.</p> Show the complete benchmark file <pre><code>package perf\n\nimport (\n    \"sync\"\n    \"testing\"\n)\n\n// types-simple-start\ntype PoorlyAligned struct {\n    flag bool\n    count int64\n    id byte\n}\n\ntype WellAligned struct {\n    count int64\n    flag bool\n    id byte\n}\n// types-simple-end\n\n// simple-start\nfunc BenchmarkPoorlyAligned(b *testing.B) {\n    for b.Loop() {\n        var items = make([]PoorlyAligned, 10_000_000)\n        for j := range items {\n            items[j].count = int64(j)\n        }\n    }\n}\n\nfunc BenchmarkWellAligned(b *testing.B) {\n    for b.Loop() {\n        var items = make([]WellAligned, 10_000_000)\n        for j := range items {\n            items[j].count = int64(j)\n        }\n    }\n}\n// simple-end\n\n\n// types-shared-start\ntype SharedCounterBad struct {\n    a int64\n    b int64\n}\n\ntype SharedCounterGood struct {\n    a int64\n    _ [56]byte // Padding to prevent a and b from sharing a cache line\n    b int64\n}\n// types-shared-end\n\n// shared-start\n\nfunc BenchmarkFalseSharing(b *testing.B) {\n    var c SharedCounterBad  // (1)\n    var wg sync.WaitGroup\n\n    for b.Loop() {\n        wg.Add(2)\n        go func() {\n            for i := 0; i &lt; 1_000_000; i++ {\n                c.a++\n            }\n            wg.Done()\n        }()\n        go func() {\n            for i := 0; i &lt; 1_000_000; i++ {\n                c.b++\n            }\n            wg.Done()\n        }()\n        wg.Wait()\n    }\n}\n// shared-end\n\nfunc BenchmarkNoFalseSharing(b *testing.B) {\n    var c SharedCounterGood\n    var wg sync.WaitGroup\n\n    for b.Loop() {\n        wg.Add(2)\n        go func() {\n            for i := 0; i &lt; 1_000_000; i++ {\n                c.a++\n            }\n            wg.Done()\n        }()\n        go func() {\n            for i := 0; i &lt; 1_000_000; i++ {\n                c.b++\n            }\n            wg.Done()\n        }()\n        wg.Wait()\n    }\n}\n</code></pre>"},{"location":"01-common-patterns/fields-alignment/#when-to-align-structs","title":"When To Align Structs","text":"<p> Always align structs. It's free to implement and often leads to better memory efficiency without changing any logic\u2014only field order needs to be adjusted.</p> <p>Guidelines for struct alignment:</p> <ul> <li>Order fields by decreasing size to reduce internal padding. Larger fields first help prevent unnecessary gaps caused by alignment rules.</li> <li>Group same-sized fields together to optimize memory layout. This ensures fields can be packed tightly without additional padding.</li> <li>Use padding deliberately to separate fields accessed by different goroutines. Preventing false sharing can improve performance in concurrent applications.</li> <li>Avoid interleaving small and large fields. Mixing sizes leads to inefficient memory usage due to extra alignment padding between fields.</li> <li>Use the fieldalignment linter to verify. This tool helps catch suboptimal layouts automatically during development.</li> </ul>"},{"location":"01-common-patterns/gc/","title":"Memory Efficiency: Mastering Go\u2019s Garbage Collector","text":"<p>Memory management in Go is automated\u2014but it\u2019s not invisible. Every allocation you make contributes to GC workload. The more frequently objects are created and discarded, the more work the runtime has to do reclaiming memory.</p> <p>This becomes especially relevant in systems prioritizing low latency, predictable resource usage, or high throughput. Tuning your allocation patterns and leveraging newer features like weak references can help reduce pressure on the GC without adding complexity to your code.</p>"},{"location":"01-common-patterns/gc/#how-gos-garbage-collector-works","title":"How Go's Garbage Collector Works","text":"<p>Info</p> <p>Highly encourage you to read the official A Guide to the Go Garbage Collector! The document provides a detailed description of multiple Go's GC internals.</p> <p>Go uses a non-generational, concurrent, tri-color mark-and-sweep garbage collector. Here's what that means in practice and how it's implemented.</p>"},{"location":"01-common-patterns/gc/#non-generational","title":"Non-generational","text":"<p>Many modern GCs, like those in the JVM or .NET CLR, divide memory into generations (young and old) under the assumption that most objects die young. These collectors focus on the young generation, which leads to shorter collection cycles.</p> <p>Go\u2019s GC takes a different approach. It treats all objects equally\u2014no generational segmentation\u2014not because generational GC conflicts with short pause times or concurrent scanning, but because it hasn\u2019t shown clear, consistent benefits in real-world Go programs with the designs tried so far. This choice avoids the complexity of promotion logic and specialized memory regions. While it can mean scanning more objects overall, this cost is mitigated by concurrent execution and efficient write barriers.</p>"},{"location":"01-common-patterns/gc/#concurrent","title":"Concurrent","text":"<p>Go\u2019s GC runs concurrently with your application, which means it does most of its work without stopping the world. Concurrency is implemented using multiple phases that interleave with normal program execution:</p> <p>Even though Go\u2019s garbage collector is mostly concurrent, it still requires brief Stop-The-World (STW) pauses at several points to maintain correctness. These pauses are kept extremely short\u2014typically under 100 microseconds\u2014even with large heaps and hundreds of goroutines.</p> <p>STW is essential for ensuring that memory structures are not mutated while the GC analyzes them. In most applications, these pauses are imperceptible. However, even sub-millisecond pauses in latency-sensitive systems can be significant\u2014so understanding and monitoring STW behavior becomes important when optimizing for tail latencies or jitter.</p> <ul> <li>STW Start Phase: The application is briefly paused to initiate GC. The runtime scans stacks, globals, and root objects.</li> <li>Concurrent Mark Phase: The GC walks the heap graph and marks reachable objects in parallel with your program. This is the most substantial phase and runs concurrently with minimal interference.</li> <li>STW Mark Termination: A short pause occurs to finalize marking and ensure consistency.</li> <li>Concurrent Sweep Phase: The GC reclaims memory from unreachable (white) objects and returns it to the heap for reuse, all while your program continues running.</li> </ul> <p>Write barriers ensure correctness while the application mutates objects during concurrent marking. These barriers help track references created or modified mid-scan so the GC doesn\u2019t miss them.</p>"},{"location":"01-common-patterns/gc/#tri-color-mark-and-sweep","title":"Tri-color Mark and Sweep","text":"<p>The tri-color algorithm organizes heap objects into three sets:</p> <ul> <li>White: Unreachable objects (candidates for collection)</li> <li>Grey: Reachable but not fully scanned (discovered but not processed)</li> <li>Black: Reachable and fully scanned (safe from collection)</li> </ul> <p>The GC begins by marking root objects as grey. It then processes each grey object, scanning its fields:</p> <ul> <li>Any referenced objects not already marked are added to the grey set.</li> <li>Once all references are scanned, the object is turned black.</li> </ul> <p>Objects left white at the end of the mark phase are unreachable and swept during the sweep phase.</p> <p>A key optimization is incremental marking: Go spreads out GC work to avoid long pauses, supported by precise stack scanning and conservative write barriers. The use of concurrent sweeping further reduces latency, allowing memory to be reclaimed without halting execution.</p> <p>This design gives Go a GC that\u2019s safe, fast, and friendly to server workloads with large heaps and many cores.</p>"},{"location":"01-common-patterns/gc/#gc-tuning-gogc","title":"GC Tuning: GOGC","text":"<p>Go\u2019s garbage collector is well-tuned out of the box. In most cases, the default setting for <code>GOGC</code> provides a solid balance between memory usage and CPU overhead. Manual GC tuning is unnecessary\u2014and often counterproductive for most workloads, especially general-purpose services.</p> <p>That said, there are specific cases where tuning <code>GOGC</code> can yield significant gains. For example, Uber implemented dynamic GC tuning across their Go services to reduce CPU usage and saved tens of thousands of cores in the process. Their approach relied on profiling, metric collection, and automation to safely adjust GC behavior based on actual memory pressure and workload characteristics.</p> <p>Another unusual case is from Cloudflare. They profiled a high-concurrency cryptographic workload and found that Go\u2019s GC became a bottleneck as goroutines increased. Their application produced minimal garbage, yet GC overhead grew with concurrency. By tuning GOGC to a much higher value\u2014specifically 11300\u2014they significantly reduced GC frequency and improved throughput, achieving over 22\u00d7 performance gains compared to the single-core baseline. This case highlights how allowing more heap growth in CPU-bound and low-allocation scenarios can yield major improvements.</p> <p>So, if you decide to tune the garbage collector, be methodical:</p> <ul> <li>Always profile first. Use tools like <code>pprof</code> to confirm that GC activity is a bottleneck.</li> <li>Change settings incrementally. For example, increasing <code>GOGC</code> from 100 to 150 means the GC will run less frequently, using less CPU but more memory.</li> <li>Verify impact. After tuning, validate with profiling data that the change had a positive effect. Without that confirmation, it's easy to make things worse.</li> </ul> <pre><code>GOGC=100  # Default: GC runs when heap grows 100% since last collection\nGOGC=off  # Disables GC (use only in special cases like short-lived CLI tools)\n</code></pre>"},{"location":"01-common-patterns/gc/#memory-limiting-with-gomemlimit","title":"Memory Limiting with <code>GOMEMLIMIT</code>","text":"<p>In addition to <code>GOGC</code>, Go provides <code>GOMEMLIMIT</code>\u2014a soft memory limit that caps the total heap size the runtime will try to stay under. This allows you to explicitly control memory growth, especially useful in environments like containers or systems with strict memory budgets.</p> <p>Why is this helpful? In containerized environments (like Kubernetes), memory limits are typically enforced at the OS or orchestrator level. If your application exceeds its memory quota, the OOM killer may abruptly terminate the container. Go's GC isn't aware of those limits by default.</p> <p>Setting a <code>GOMEMLIMIT</code> helps prevent this. For example, if your container has a 512MiB memory limit, you might set:</p> <pre><code>GOMEMLIMIT=400MiB\n</code></pre> <p>This gives the Go runtime a safe buffer to start collecting more aggressively before hitting the system-enforced limit, reducing the risk of termination. It also ensures there's headroom for non-heap allocations such as goroutine stacks and internal runtime data.</p> <p>You can also set the limit programmatically:</p> <pre><code>import \"runtime/debug\"\n\ndebug.SetMemoryLimit(2 &lt;&lt; 30) // 2 GiB\n</code></pre> <p>The GC will become more aggressive as heap usage nears the limit, which can increase CPU load. Be careful not to set the limit too low\u2014especially if your application maintains a large live set of objects\u2014or you may trigger excessive GC cycles.</p> <p>While <code>GOGC</code> controls how frequently the GC runs based on heap growth, <code>GOMEMLIMIT</code> constrains the heap size itself. The two can be combined for more precise control:</p> <pre><code>GOGC=100 GOMEMLIMIT=4GiB ./your-service\n</code></pre> <p>This tells the GC to operate with the default growth ratio and to start collecting sooner if heap usage nears 4 GiB.</p>"},{"location":"01-common-patterns/gc/#gomemlimitx-and-gogcoff-configuration","title":"GOMEMLIMIT=X and GOGC=off configuration","text":"<p>In scenarios where memory availability is fixed and predictable\u2014such as within containers or VMs, you can use these two variables together:</p> <ul> <li><code>GOMEMLIMIT=X</code> tells the runtime to aim for a specific memory ceiling. For example, <code>GOMEMLIMIT=2GiB</code> will trigger garbage collection when total memory usage nears 2 GiB.</li> <li><code>GOGC=off</code> disables the default GC pacing algorithm, so garbage collection only runs when the memory limit is hit.</li> </ul> <p>This configuration maximizes memory usage efficiency and avoids the overhead of frequent GC cycles. It's especially effective in high-throughput or latency-sensitive systems where predictable memory usage matters.</p> <p>Example:</p> <pre><code>GOMEMLIMIT=2GiB GOGC=off ./my-app\n</code></pre> <p>With this setup, memory usage grows freely until the 2 GiB threshold is reached. At that point, Go performs a full garbage collection pass.</p> <p>Warning</p> <ul> <li>Always benchmark with your real workload. Disabling automatic GC can backfire if your application produces a lot of short-lived allocations.</li> <li>Monitor memory pressure and GC pause times using <code>runtime.ReadMemStats</code> or <code>pprof</code>.</li> <li>This approach works best when your memory usage patterns are well understood and stable.</li> </ul>"},{"location":"01-common-patterns/gc/#practical-strategies-for-reducing-gc-pressure","title":"Practical Strategies for Reducing GC Pressure","text":""},{"location":"01-common-patterns/gc/#prefer-stack-allocation","title":"Prefer Stack Allocation","text":"<p>Go allocates variables on the stack whenever possible. Avoid escaping variables to the heap:</p> <pre><code>// BAD: returns pointer to heap-allocated struct\nfunc newUser(name string) *User {\n    return &amp;User{Name: name}  // escapes to heap\n}\n\n// BETTER: use value types if pointer is unnecessary\nfunc printUser(u User) {\n    fmt.Println(u.Name)\n}\n</code></pre> <p>Use <code>go build -gcflags=\"-m\"</code> to view escape analysis diagnostics. See Stack Allocations and Escape Analysis for more details.</p>"},{"location":"01-common-patterns/gc/#use-syncpool-for-short-lived-objects","title":"Use sync.Pool for Short-Lived Objects","text":"<p><code>sync.Pool</code> is ideal for temporary, reusable allocations that are expensive to GC.</p> <pre><code>var bufPool = sync.Pool{\n    New: func() any { return new(bytes.Buffer) },\n}\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    buf := bufPool.Get().(*bytes.Buffer)\n    buf.Reset()\n    defer bufPool.Put(buf)\n\n    // Use buf...\n}\n</code></pre> <p>See Object Pooling for more details.</p>"},{"location":"01-common-patterns/gc/#batch-allocations","title":"Batch Allocations","text":"<p>Group allocations into fewer objects to reduce GC pressure.</p> <pre><code>// Instead of allocating many small structs, allocate a slice of structs\nusers := make([]User, 0, 1000)  // single large allocation\n</code></pre> <p>See Memory Preallocation for more details.</p>"},{"location":"01-common-patterns/gc/#weak-references-in-go","title":"Weak References in Go","text":"<p>Go 1.24 introduced the <code>weak</code> package, which offers a safer and standardized way to create weak references. Weak references allow you to reference an object without preventing it from being garbage collected. In typical garbage-collected systems, holding a strong reference to an object ensures that it remains alive. This can lead to unintended memory retention\u2014especially in caches, graphs, or deduplication maps\u2014where references are kept long after the object is actually needed.</p> <p>A weak reference, by contrast, tells the garbage collector: \u201cyou can collect this object if nothing else is strongly referencing it.\u201d This pattern is important for building memory-sensitive data structures that should not interfere with garbage collection.</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"runtime\"\n    \"weak\"\n)\n\ntype Data struct {\n    Value string\n}\n\nfunc main() {\n    data := &amp;Data{Value: \"Important\"}\n    wp := weak.Make(data) // create weak pointer\n\n    fmt.Println(\"Original:\", wp.Value().Value)\n\n    data = nil // remove strong reference\n    runtime.GC()\n\n    if v := wp.Value(); v != nil {\n        fmt.Println(\"Still alive:\", v.Value)\n    } else {\n        fmt.Println(\"Data has been collected\")\n    }\n}\n</code></pre> <pre><code>Original: Important\nData has been collected\n</code></pre> <p>Here, wp holds a weak reference to Session. Once the strong reference (s) is dropped and GC runs, the session object can be collected, and wp.Value() will return nil. This is particularly useful for memory-sensitive structures like caches or canonicalization maps. Always check the result of <code>Value()</code> to confirm the object is still valid.</p>"},{"location":"01-common-patterns/gc/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>It's tempting to rely on synthetic benchmarks to evaluate the performance of Go's garbage collector, but generic benchmarks rarely capture the nuances of real-world workloads. Memory behavior is highly dependent on allocation patterns, object lifetimes, concurrency, and how frequently short-lived versus long-lived data structures are used.</p> <p>For example, the impact of GC in a CPU-bound microservice that maintains large in-memory indexes will differ dramatically from an I/O-heavy API server with minimal heap usage. As such, tuning decisions should always be informed by your application's profiling data.</p> <p>We cover targeted use cases and their GC performance trade-offs in more focused articles:</p> <ul> <li>Object Pooling: Reducing allocation churn using <code>sync.Pool</code></li> <li>Stack Allocations and Escape Analysis: Minimizing heap usage by keeping values on the stack</li> <li>Memory Preallocation: Avoiding unnecessary growth of slices and maps</li> </ul> <p>When applied to the right context, these techniques can make a measurable difference, but they don\u2019t lend themselves to one-size-fits-all benchmarks.</p>"},{"location":"01-common-patterns/immutable-data/","title":"Immutable Data Sharing","text":"<p>One common bottleneck when building high-performance Go applications is concurrent access to shared data. The traditional approach often involves mutexes or channels to manage synchronization. While effective, these tools can add complexity and subtle bugs if not used carefully.</p> <p>A powerful alternative is immutable data sharing. Instead of protecting data with locks, you design your system so that shared data is never mutated after it's created. This minimizes contention and simplifies reasoning about your program.</p>"},{"location":"01-common-patterns/immutable-data/#why-immutable-data","title":"Why Immutable Data?","text":"<p>Immutability brings several advantages to concurrent programs:</p> <ul> <li>No locks needed: Multiple goroutines can safely read immutable data without synchronization.</li> <li>Easier reasoning: If data can't change, you avoid entire classes of race conditions.</li> <li>Copy-on-write optimizations: You can create new versions of a structure without altering the original, which is useful for config reloading or versioning a state.</li> </ul>"},{"location":"01-common-patterns/immutable-data/#practical-example-shared-config","title":"Practical Example: Shared Config","text":"<p>Imagine you have a long-running service that periodically reloads its configuration from a disk or a remote source. Multiple goroutines read this configuration to make decisions.</p> <p>Here's how immutable data helps:</p>"},{"location":"01-common-patterns/immutable-data/#step-1-define-the-config-struct","title":"Step 1: Define the Config Struct","text":"<pre><code>// config.go\ntype Config struct {\n    LogLevel string\n    Timeout  time.Duration\n    Features map[string]bool // This needs attention!\n}\n</code></pre>"},{"location":"01-common-patterns/immutable-data/#step-2-ensure-deep-immutability","title":"Step 2: Ensure Deep Immutability","text":"<p>Maps and slices in Go are reference types. Even if the Config struct isn't changed, someone could accidentally mutate a shared map. To prevent this, we make defensive copies:</p> <pre><code>func NewConfig(logLevel string, timeout time.Duration, features map[string]bool) *Config {\n    copiedFeatures := make(map[string]bool, len(features))\n    for k, v := range features {\n        copiedFeatures[k] = v\n    }\n\n    return &amp;Config{\n        LogLevel: logLevel,\n        Timeout:  timeout,\n        Features: copiedFeatures,\n    }\n}\n</code></pre> <p>Now, every config instance is self-contained and safe to share.</p>"},{"location":"01-common-patterns/immutable-data/#step-3-atomic-swapping","title":"Step 3: Atomic Swapping","text":"<p>Use <code>atomic.Value</code> to store and safely update the current config.</p> <pre><code>var currentConfig atomic.Pointer[Config]\n\nfunc LoadInitialConfig() {\n    cfg := NewConfig(\"info\", 5*time.Second, map[string]bool{\"beta\": true})\n    currentConfig.Store(cfg)\n}\n\nfunc GetConfig() *Config {\n    return currentConfig.Load()\n}\n</code></pre> <p>Now all goroutines can safely call <code>GetConfig()</code> with no locks. When the config is reloaded, you just <code>Store</code> a new immutable copy.</p>"},{"location":"01-common-patterns/immutable-data/#step-4-using-it-in-handlers","title":"Step 4: Using It in Handlers","text":"<pre><code>func handler(w http.ResponseWriter, r *http.Request) {\n    cfg := GetConfig()\n    if cfg.Features[\"beta\"] {\n        // Enable beta path\n    }\n    // Use cfg.Timeout, cfg.LogLevel, etc.\n}\n</code></pre>"},{"location":"01-common-patterns/immutable-data/#practical-example-immutable-routing-table","title":"Practical Example: Immutable Routing Table","text":"<p>Suppose you're building a lightweight reverse proxy or API gateway and must route incoming requests based on path or host. The routing table is read thousands of times per second and updated only occasionally (e.g., from a config file or service discovery).</p>"},{"location":"01-common-patterns/immutable-data/#step-1-define-route-structs","title":"Step 1: Define Route Structs","text":"<pre><code>type Route struct {\n    Path    string\n    Backend string\n}\n\ntype RoutingTable struct {\n    Routes []Route\n}\n</code></pre>"},{"location":"01-common-patterns/immutable-data/#step-2-build-immutable-version","title":"Step 2: Build Immutable Version","text":"<p>To ensure immutability, we deep-copy the slice of routes when constructing a new routing table.</p> <pre><code>func NewRoutingTable(routes []Route) *RoutingTable {\n    copied := make([]Route, len(routes))\n    copy(copied, routes)\n    return &amp;RoutingTable{Routes: copied}\n}\n</code></pre>"},{"location":"01-common-patterns/immutable-data/#step-3-store-it-atomically","title":"Step 3: Store It Atomically","text":"<pre><code>var currentRoutes atomic.Pointer[RoutingTable]\n\nfunc LoadInitialRoutes() {\n    table := NewRoutingTable([]Route{\n        {Path: \"/api\", Backend: \"http://api.internal\"},\n        {Path: \"/admin\", Backend: \"http://admin.internal\"},\n    })\n    currentRoutes.Store(table)\n}\n\nfunc GetRoutingTable() *RoutingTable {\n    return currentRoutes.Load()\n}\n</code></pre>"},{"location":"01-common-patterns/immutable-data/#step-4-route-requests-concurrently","title":"Step 4: Route Requests Concurrently","text":"<pre><code>func routeRequest(path string) string {\n    table := GetRoutingTable()\n    for _, route := range table.Routes {\n        if strings.HasPrefix(path, route.Path) {\n            return route.Backend\n        }\n    }\n    return \"\"\n}\n</code></pre> <p>Now, your routing logic can scale safely under load with zero locking overhead.</p>"},{"location":"01-common-patterns/immutable-data/#scaling-immutable-routing-tables","title":"Scaling Immutable Routing Tables","text":"<p>As your system grows, the routing table might contain hundreds or even thousands of rules. Rebuilding and copying the entire structure every minor change might no longer be practical.</p> <p>Let\u2019s consider a few ways to evolve this design while keeping the benefits of immutability.</p>"},{"location":"01-common-patterns/immutable-data/#scenario-1-segmented-routing","title":"Scenario 1: Segmented Routing","text":"<p>Imagine a multi-tenant system where each customer has their own set of routing rules. Instead of one giant slice of routes, you can split them into a map:</p> <pre><code>type MultiTable struct {\n    Tables map[string]RoutingTable // key = tenant ID\n}\n</code></pre> <p>If only customer \"acme\" updates their rules, you clone just that slice and update the map. Then you atomically swap in a new version of the full map. All other tenants continue using their existing, untouched routing tables.</p> <p>This approach reduces memory pressure and speeds up updates without losing immutability. It also isolates blast radius: a broken rule set in one segment doesn\u2019t affect others.</p>"},{"location":"01-common-patterns/immutable-data/#scenario-2-indexed-routing-table","title":"Scenario 2: Indexed Routing Table","text":"<p>Let\u2019s say your router matches by exact path, and lookup speed is critical. You can use a <code>map[string]RouteHandler</code> as an index:</p> <pre><code>type RouteIndex map[string]RouteHandler\n</code></pre> <p>When a new path is added, clone the current map, add the new route, and publish the new version. Because maps are shallow, this is fast for moderate numbers of routes. Reads are constant time, and updates are efficient because only a small part of the structure changes.</p>"},{"location":"01-common-patterns/immutable-data/#scenario-3-hybrid-staging-and-publishing","title":"Scenario 3: Hybrid Staging and Publishing","text":"<p>Suppose you\u2019re doing a batch update \u2014 maybe reading hundreds of routes from a database. Instead of rebuilding live, you keep a mutable staging area:</p> <pre><code>var mu sync.Mutex\nvar stagingRoutes []Route\n</code></pre> <p>You load and manipulate data in staging under a mutex, then convert to an immutable <code>RoutingTable</code> and store it atomically. This lets you safely prepare complex changes without locking readers or affecting live traffic.</p>"},{"location":"01-common-patterns/immutable-data/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>Benchmarking immutable data sharing in real-world systems is difficult to do in a generic, meaningful way. Factors like structure size, read/write ratio, and memory layout all heavily influence results.</p> <p>Rather than presenting artificial benchmarks here, we recommend reviewing the results in the Atomic Operations and Synchronization Primitives article. Those benchmarks clearly illustrate the potential performance benefits of using atomic.Value over traditional synchronization primitives like sync.RWMutex, especially in highly concurrent read scenarios.</p>"},{"location":"01-common-patterns/immutable-data/#when-to-use-this-pattern","title":"When to Use This Pattern","text":"<p> Immutable data sharing is ideal when:</p> <ul> <li> <p>The data is read-heavy and write-light (e.g., configuration, feature flags, global mappings). This works well because the cost of creating new immutable versions is amortized over many reads, and avoiding locks provides a performance boost.</p> </li> <li> <p>You want to minimize locking without sacrificing safety. By sharing read-only data, you remove the need for mutexes or coordination, reducing the chances of deadlocks or race conditions.</p> </li> <li> <p>You can tolerate minor delays between update and read (eventual consistency). Since data updates are not coordinated with readers, there might be a small delay before all goroutines see the new version. If exact timing isn't critical, this tradeoff simplifies your concurrency model.</p> </li> </ul> <p> It\u2019s less suitable when updates must be transactional across multiple pieces of data or happen frequently. In those cases, the cost of repeated copying or lack of coordination can outweigh the benefits.</p>"},{"location":"01-common-patterns/interface-boxing/","title":"Avoiding Interface Boxing","text":"<p>Go\u2019s interfaces make it easy to write flexible, decoupled code. But behind that convenience is a detail that can trip up performance: when a concrete value is assigned to an interface, Go wraps it in a hidden structure\u2014a process called interface boxing.</p> <p>In many cases, boxing is harmless. But in performance-sensitive code\u2014like tight loops, hot paths, or high-throughput services\u2014it can introduce hidden heap allocations, extra memory copying, and added pressure on the garbage collector. These effects often go unnoticed during development, only showing up later as latency spikes or memory bloat.</p>"},{"location":"01-common-patterns/interface-boxing/#what-is-interface-boxing","title":"What is Interface Boxing?","text":"<p>Interface boxing refers to the process of converting a concrete value to an interface type. In Go, an interface value consists of two parts internally:</p> <ul> <li>A type descriptor, which describes the concrete type.</li> <li>A data pointer, which points to the actual value.</li> </ul> <p>When you assign a value to an interface variable, Go creates this two-word structure under the hood. If the value is a non-pointer (a struct or a primitive) and is not already heap-allocated, Go may allocate a copy of it on the heap. This is especially relevant when the value is large or when it's stored in a slice of interfaces.</p> <p>Here\u2019s a simple example:</p> <pre><code>var i interface{}\ni = 42\n</code></pre> <p>In this case, the integer <code>42</code> is boxed into an interface: Go stores the type information (<code>int</code>) and a copy of the value <code>42</code>. This is inexpensive for small values like <code>int</code>, but for large structs, the cost becomes non-trivial.</p> <p>Another example:</p> <pre><code>type Shape interface {\n    Area() float64\n}\n\ntype Square struct {\n    Size float64\n}\n\nfunc (s Square) Area() float64 { return s.Size * s.Size }\n\nfunc main() {\n    var shapes []Shape\n    for i := 0; i &lt; 1000; i++ {\n        s := Square{Size: float64(i)}\n        shapes = append(shapes, s) // boxing occurs here\n    }\n}\n</code></pre> <p>Warning</p> <p>Pay attention to this code! In this example, even though <code>shapes</code> is a slice of interfaces, each <code>Square</code> value is copied into an interface when appended to <code>shapes</code>. If <code>Square</code> were a large struct, this would introduce 1000 allocations and large memory copying.</p> <p>To avoid that, you could pass pointers:</p> <pre><code>        shapes = append(shapes, &amp;s) // avoids large struct copy\n</code></pre> <p>This way, only an 8-byte pointer is stored in the interface, reducing both allocation size and copying overhead.</p>"},{"location":"01-common-patterns/interface-boxing/#why-it-matters","title":"Why It Matters","text":"<p>In tight loops or high-throughput paths, such as unmarshalling JSON, rendering templates, or processing large collections, interface boxing can degrade performance by triggering unnecessary heap allocations and increasing GC pressure. This overhead is especially costly in systems with high concurrency or real-time responsiveness constraints.</p> <p>Boxing can also make profiling and benchmarking misleading, since allocations attributed to innocuous-looking lines may actually stem from implicit conversions to interfaces.</p>"},{"location":"01-common-patterns/interface-boxing/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>For the benchmarking we will define an interface and a struct with a significant payload that implements the interface.</p> <pre><code>type Worker interface {\n    Work()\n}\n\ntype LargeJob struct {\n    payload [4096]byte\n}\n\nfunc (LargeJob) Work() {}\n</code></pre>"},{"location":"01-common-patterns/interface-boxing/#boxing-large-structs","title":"Boxing Large Structs","text":"<p>To demonstrate the real impact of boxing large values vs. pointers, we benchmarked the cost of assigning 1,000 large structs to an interface slice:</p> <pre><code>func BenchmarkBoxedLargeSlice(b *testing.B) {\n    jobs := make([]Worker, 0, 1000)\n    for b.Loop() {\n        jobs = jobs[:0]\n        for j := 0; j &lt; 1000; j++ {\n            var job LargeJob\n            jobs = append(jobs, job)\n        }\n    }\n}\n\nfunc BenchmarkPointerLargeSlice(b *testing.B) {\n    jobs := make([]Worker, 0, 1000)\n    for b.Loop() {\n        jobs := jobs[:0]\n        for j := 0; j &lt; 1000; j++ {\n            job := &amp;LargeJob{}\n            jobs = append(jobs, job)\n        }\n    }\n}\n</code></pre> <p>Benchmark Results</p> Benchmark Time per op (ns) Bytes per op Allocs per op BoxedLargeSliceGrowth 404,649 ~4.13 MB 1011 PointerLargeSliceGrowth 340,549 ~4.13 MB 1011 <p>Boxing large values is significantly slower\u2014about 19% in this case\u2014due to the cost of copying the entire 4KB struct for each interface assignment. Boxing a pointer, however, avoids that cost and keeps the copy small (just 8 bytes). While both approaches allocate the same overall memory (since all values escape to the heap), pointer boxing has clear performance advantages under pressure.</p>"},{"location":"01-common-patterns/interface-boxing/#passing-to-a-function-that-accepts-an-interface","title":"Passing to a Function That Accepts an Interface","text":"<p>Another common source of boxing is when a large value is passed directly to a function that accepts an interface. Even without storing to a slice, boxing will occur at the call site.</p> <pre><code>var sink Worker\n\nfunc call(w Worker) {\n    sink = w\n}\n\nfunc BenchmarkCallWithValue(b *testing.B) {\n    for b.Loop() {\n        var j LargeJob\n        call(j)\n    }\n}\n\nfunc BenchmarkCallWithPointer(b *testing.B) {\n    for b.Loop() {\n        j := &amp;LargeJob{}\n        call(j)\n    }\n}\n</code></pre> <p>Benchmark Results</p> Benchmark ns/op B/op allocs/op CallWithValue 422.5 4096 1 CallWithPointer 379.9 4096 1 <p>Passing a value to a function expecting an interface causes boxing, copying the full struct and allocating it on the heap. In our benchmark, this results in approximately 11% higher CPU cost compared to using a pointer. Passing a pointer avoids copying the struct, reduces memory movement, and results in smaller, more cache-friendly interface values, making it the more efficient choice in performance-sensitive scenarios.</p> Show the complete benchmark file <pre><code>package perf\n\nimport \"testing\"\n\n\n// interface-start\n\ntype Worker interface {\n    Work()\n}\n\ntype LargeJob struct {\n    payload [4096]byte\n}\n\nfunc (LargeJob) Work() {}\n// interface-end\n\n// bench-slice-start\nfunc BenchmarkBoxedLargeSlice(b *testing.B) {\n    jobs := make([]Worker, 0, 1000)\n    for b.Loop() {\n        jobs = jobs[:0]\n        for j := 0; j &lt; 1000; j++ {\n            var job LargeJob\n            jobs = append(jobs, job)\n        }\n    }\n}\n\nfunc BenchmarkPointerLargeSlice(b *testing.B) {\n    jobs := make([]Worker, 0, 1000)\n    for b.Loop() {\n        jobs := jobs[:0]\n        for j := 0; j &lt; 1000; j++ {\n            job := &amp;LargeJob{}\n            jobs = append(jobs, job)\n        }\n    }\n}\n// bench-slice-end\n\n// bench-call-start\nvar sink Worker\n\nfunc call(w Worker) {\n    sink = w\n}\n\nfunc BenchmarkCallWithValue(b *testing.B) {\n    for b.Loop() {\n        var j LargeJob\n        call(j)\n    }\n}\n\nfunc BenchmarkCallWithPointer(b *testing.B) {\n    for b.Loop() {\n        j := &amp;LargeJob{}\n        call(j)\n    }\n}\n// bench-call-end\n</code></pre>"},{"location":"01-common-patterns/interface-boxing/#when-interface-boxing-is-acceptable","title":"When Interface Boxing Is Acceptable","text":"<p>Despite its performance implications in some contexts, interface boxing is often perfectly reasonable\u2014and sometimes preferred.</p>"},{"location":"01-common-patterns/interface-boxing/#when-abstraction-is-more-important-than-performance","title":"When abstraction is more important than performance","text":"<p>Interfaces enable decoupling and modularity. If you're designing a clean, testable API, the cost of boxing is negligible compared to the benefit of abstraction.</p> <pre><code>type Storage interface {\n    Save([]byte) error\n}\nfunc Process(s Storage) { /* ... */ }\n</code></pre>"},{"location":"01-common-patterns/interface-boxing/#when-values-are-small-and-boxing-is-allocation-free","title":"When values are small and boxing is allocation-free","text":"<p>Boxing small, copyable values like <code>int</code>, <code>float64</code>, or small structs typically causes no allocations.</p> <pre><code>var i interface{}\ni = 123 // safe and cheap\n</code></pre>"},{"location":"01-common-patterns/interface-boxing/#when-values-are-short-lived","title":"When values are short-lived","text":"<p>If the boxed value is used briefly (e.g. for logging or interface-based sorting), the overhead is minimal.</p> <pre><code>fmt.Println(\"value:\", someStruct) // implicit boxing is fine\n</code></pre>"},{"location":"01-common-patterns/interface-boxing/#when-dynamic-behavior-is-required","title":"When dynamic behavior is required","text":"<p>Interfaces allow runtime polymorphism. If you need different types to implement the same behavior, boxing is necessary and idiomatic.</p> <pre><code>for _, s := range []Shape{Circle{}, Square{}} {\n    fmt.Println(s.Area())\n}\n</code></pre> <p>Use boxing when it supports clarity, reusability, or design goals\u2014and avoid it only in performance-critical code paths.</p>"},{"location":"01-common-patterns/interface-boxing/#how-to-avoid-interface-boxing","title":"How to Avoid Interface Boxing","text":"<ul> <li>Use pointers when assigning to interfaces. If the method set requires a pointer receiver or the value is large, explicitly pass a pointer to avoid repeated copying and heap allocation.     <pre><code>for i := range tasks {\n   result = append(result, &amp;tasks[i]) // Avoids boxing copies\n}\n</code></pre></li> <li>Avoid interfaces in hot paths. If the concrete type is known and stable, avoid interface indirection entirely\u2014especially in compute-intensive or allocation-sensitive functions.</li> <li>Use type-specific containers. Instead of <code>[]interface{}</code>, prefer generic slices or typed collections where feasible. This preserves static typing and reduces unnecessary allocations.</li> <li>Benchmark and inspect with pprof. Use <code>go test -bench</code> and <code>pprof</code> to observe where allocations occur. If the allocation site is in <code>runtime.convT2E</code> (convert T to interface), you're likely boxing.</li> </ul>"},{"location":"01-common-patterns/lazy-init/","title":"Lazy Initialization","text":""},{"location":"01-common-patterns/lazy-init/#lazy-initialization-for-performance-in-go","title":"Lazy Initialization for Performance in Go","text":"<p>In Go, some resources are expensive to initialize, or simply unnecessary unless certain code paths are triggered. That\u2019s where lazy initialization becomes useful: it defers the construction of a value until the moment it\u2019s actually needed. This pattern can improve performance, reduce startup overhead, and avoid unnecessary work\u2014especially in high-concurrency applications.</p>"},{"location":"01-common-patterns/lazy-init/#why-lazy-initialization-matters","title":"Why Lazy Initialization Matters","text":"<p>Initializing complex resources\u2014such as database connections, caches, or large data structures\u2014at application startup can significantly delay launch time and unnecessarily consume memory. Lazy initialization ensures these resources are only created when needed, optimizing resource usage and performance.</p> <p>Additionally, lazy initialization is crucial when you have code that might be executed multiple times, but you need a resource or logic executed precisely once. This pattern helps ensure idempotency and avoids redundant processing.</p>"},{"location":"01-common-patterns/lazy-init/#using-synconce-for-thread-safe-initialization","title":"Using <code>sync.Once</code> for Thread-Safe Initialization","text":"<p>Go provides the <code>sync.Once</code> type to implement lazy initialization safely in concurrent environments:</p> <pre><code>var (\n    resource *MyResource\n    once     sync.Once\n)\n\nfunc getResource() *MyResource {\n    once.Do(func() {\n        resource = expensiveInit()\n    })\n    return resource\n}\n</code></pre> <p>In this example, the function <code>expensiveInit()</code> executes exactly once, no matter how many goroutines invoke <code>getResource()</code> concurrently. This ensures thread-safe initialization without additional synchronization overhead.</p>"},{"location":"01-common-patterns/lazy-init/#using-synconcevalue-and-synconcevalues-for-initialization-with-output-values","title":"Using <code>sync.OnceValue</code> and <code>sync.OnceValues</code> for Initialization with Output Values","text":"<p>Since Go 1.21, if your initialization logic returns a value, you might prefer using <code>sync.OnceValue</code> (single value) or <code>sync.OnceValues</code> (multiple values) for simpler, more expressive code:</p> <pre><code>var getResource = sync.OnceValue(func() *MyResource {\n    return expensiveInit()\n})\n\nfunc processData() {\n    res := getResource()\n    // use res\n}\n</code></pre> <p>Here, <code>sync.OnceValue</code> neatly encapsulates initialization logic and directly returns the initialized value, eliminating explicit state management.</p> <p>For scenarios where your initialization returns multiple values, <code>sync.OnceValues</code> offers a clean and efficient solution:</p> <pre><code>var getConfig = sync.OnceValues(func() (*Config, error) {\n    return loadConfig(\"config.yml\")\n})\n\nfunc processData() {\n    config, err := getConfig()\n    if err != nil {\n        log.Fatal(err)\n    }\n    // use config\n}\n</code></pre> <p>Choosing <code>sync.OnceValue</code> or <code>sync.OnceValues</code> helps you clearly express initialization logic with direct value returns, whereas <code>sync.Once</code> remains best suited for general scenarios requiring flexible initialization logic without immediate value returns.</p>"},{"location":"01-common-patterns/lazy-init/#custom-lazy-initialization-with-atomic-operations","title":"Custom Lazy Initialization with Atomic Operations","text":"<p>Yes, it\u2019s technically possible to replace <code>sync.Once</code>, <code>sync.OnceValue</code>, or <code>sync.OnceFunc</code> with custom logic using low-level atomic operations. This approach may offer slightly finer control or avoid allocations in extremely performance-critical code paths.</p> <p>That said, it\u2019s rarely worth the tradeoff.</p> <p>Manual atomic-based initialization is more error-prone, harder to read, and easier to get wrong\u2014especially when concurrency and memory visibility guarantees are involved. For the vast majority of cases, <code>sync.Once*</code> is safer, clearer, and performant enough.</p> <p>Info</p> <p>If you\u2019re convinced that atomic-based lazy initialization is justified in your case, this blog post walks through the details and caveats: Lazy initialization in Go using atomics</p>"},{"location":"01-common-patterns/lazy-init/#performance-considerations","title":"Performance Considerations","text":"<p>While lazy initialization can offer clear benefits, it also brings added complexity. It\u2019s important to handle initialization carefully to avoid subtle issues like race conditions or concurrency bugs. Using built-in tools like <code>sync.Once</code> or <code>atomic</code> operations typically ensures thread-safety without much hassle. Still, it\u2019s always a good idea to measure actual improvements through profiling, confirming lazy initialization truly enhances startup speed, reduces memory usage, or boosts your application's responsiveness.</p>"},{"location":"01-common-patterns/lazy-init/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>There is typically nothing specific to benchmark with lazy initialization itself, as the main benefit is deferring expensive resource creation. The performance gains are inherently tied to the avoided cost of unnecessary initialization, startup speed improvements, and reduced memory consumption, rather than direct runtime throughput differences.</p>"},{"location":"01-common-patterns/lazy-init/#when-to-choose-lazy-initialization","title":"When to Choose Lazy Initialization","text":"<ul> <li>When resource initialization is costly or involves I/O. Delaying construction avoids paying the cost of setup\u2014like opening files, querying databases, or loading large structures\u2014unless it\u2019s actually needed.</li> <li>To improve startup performance and memory efficiency. Deferring work until first use allows your application to start faster and avoid allocating memory for resources that may never be used.</li> <li>When not all resources are needed immediately or at all during runtime. Lazy initialization helps you avoid initializing fields or services that only apply in specific code paths.</li> <li>To guarantee a block of code executes exactly once despite repeated calls. Using tools like <code>sync.Once</code> ensures thread-safe, one-time setup in concurrent environments.</li> </ul>"},{"location":"01-common-patterns/mem-prealloc/","title":"Memory Preallocation","text":"<p>Memory preallocation is a practical way to improve performance in Go programs that deal with growing slices or maps. By allocating enough space upfront, you can avoid the overhead of repeated resizing, which often involves memory allocation, data copying, and extra work for the garbage collector.</p> <p>In high-throughput or performance-sensitive code, preallocating memory helps keep execution predictable and efficient, especially when working with large or known workloads.</p>"},{"location":"01-common-patterns/mem-prealloc/#why-preallocation-matters","title":"Why Preallocation Matters","text":"<p>In Go, slices and maps dynamically expand to accommodate new elements. While convenient, this automatic growth introduces overhead. When a slice or map reaches its capacity, Go must allocate a new memory block and copy existing data into it. Frequent resizing operations significantly degrade performance, especially within tight loops or resource-intensive tasks.</p> <p>Go employs a specific growth strategy for slices to balance memory efficiency and performance. Initially, slice capacities double with each expansion, ensuring rapid growth. However, once a slice exceeds approximately 1024 elements, the capacity growth rate reduces to about 25%. For example, starting from a capacity of 1, slices grow sequentially to capacities of 2, 4, 8, and so forth. But after surpassing 1024 elements, the next capacity increment would typically be around 1280 rather than doubling to 2048. This controlled growth reduces memory waste but increases allocation frequency if the final slice size is predictable but not explicitly preallocated.</p> <pre><code>s := make([]int, 0)\nfor i := 0; i &lt; 10_000; i++ {\n    s = append(s, i)\n    fmt.Printf(\"Len: %d, Cap: %d\\n\", len(s), cap(s))\n}\n</code></pre> <p>Output illustrating typical growth:</p> <pre><code>Len: 1, Cap: 1\nLen: 2, Cap: 2\nLen: 3, Cap: 4\nLen: 5, Cap: 8\n...\nLen: 1024, Cap: 1024\nLen: 1025, Cap: 1280\n</code></pre>"},{"location":"01-common-patterns/mem-prealloc/#practical-preallocation-examples","title":"Practical Preallocation Examples","text":""},{"location":"01-common-patterns/mem-prealloc/#slice-preallocation","title":"Slice Preallocation","text":"<p>Without preallocation, each append operation might trigger new allocations:</p> <pre><code>// Inefficient\nvar result []int\nfor i := 0; i &lt; 10000; i++ {\n    result = append(result, i)\n}\n</code></pre> <p>This pattern causes Go to allocate larger underlying arrays repeatedly as the slice grows, resulting in memory copying and GC pressure. We can avoid that by using <code>make</code> with a specified capacity:</p> <pre><code>// Efficient\nresult := make([]int, 0, 10000)\nfor i := 0; i &lt; 10000; i++ {\n    result = append(result, i)\n}\n</code></pre> <p>If it is known that the slice will be fully populated, we can be even more efficient by avoiding bounds checks:</p> <pre><code>// Efficient\nresult := make([]int, 10000)\nfor i := range result {\n    result[i] = i\n}\n</code></pre>"},{"location":"01-common-patterns/mem-prealloc/#map-preallocation","title":"Map Preallocation","text":"<p>Maps grow similarly. By default, Go doesn\u2019t know how many elements you\u2019ll add, so it resizes the underlying structure as needed.</p> <pre><code>// Inefficient\nm := make(map[int]string)\nfor i := 0; i &lt; 10000; i++ {\n    m[i] = fmt.Sprintf(\"val-%d\", i)\n}\n</code></pre> <p>Starting with Go 1.11, you can preallocate <code>map</code> capacity too:</p> <pre><code>// Efficient\nm := make(map[int]string, 10000)\nfor i := 0; i &lt; 10000; i++ {\n    m[i] = fmt.Sprintf(\"val-%d\", i)\n}\n</code></pre> <p>This helps the runtime allocate enough internal storage upfront, avoiding rehashing and resizing costs.</p>"},{"location":"01-common-patterns/mem-prealloc/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>Here\u2019s a simple benchmark comparing appending to a preallocated slice vs. a zero-capacity slice:</p> Show the benchmark file <pre><code>package perf\n\nimport (\n    \"testing\"\n)\n\nfunc BenchmarkAppendNoPrealloc(b *testing.B) {\n    for b.Loop() {\n        var s []int\n        for j := 0; j &lt; 10000; j++ {\n            s = append(s, j)\n        }\n    }\n}\n\nfunc BenchmarkAppendWithPrealloc(b *testing.B) {\n    for b.Loop() {\n        s := make([]int, 0, 10000)\n        for j := 0; j &lt; 10000; j++ {\n            s = append(s, j)\n        }\n    }\n}\n</code></pre> <p>You\u2019ll typically observe that preallocation reduces allocations to a single one per operation and significantly improves throughput.</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkAppendNoPrealloc-14 41,727 28,539 357,626 19 BenchmarkAppendWithPrealloc-14 170,154 7,093 81,920 1"},{"location":"01-common-patterns/mem-prealloc/#when-to-preallocate","title":"When To Preallocate","text":"<p> Preallocate when:</p> <ul> <li>The number of elements in slices or maps is known or reasonably predictable. Allocating memory up front avoids the cost of repeated resizing as the data structure grows.</li> <li>Your application involves tight loops or high-throughput data processing. Preallocation reduces per-iteration overhead and helps maintain steady performance under load.</li> <li>Minimizing garbage collection overhead is crucial for your application's performance. Fewer allocations mean less work for the garbage collector, resulting in lower latency and more consistent behavior.</li> </ul> <p> Avoid preallocation when:</p> <ul> <li>The data size is highly variable and unpredictable. Allocating too much or too little memory can either waste resources or negate the performance benefit.</li> <li>Over-allocation risks significant memory waste. Reserving more memory than needed can increase your application\u2019s footprint unnecessarily.</li> <li>You're prematurely optimizing\u2014always profile to confirm the benefit. Preallocation is helpful, but only when it solves a real, measurable problem in your workload.</li> </ul>"},{"location":"01-common-patterns/object-pooling/","title":"Object Pooling","text":"<p>Object pooling is a practical way to cut down on memory allocation costs in performance-critical Go applications. Instead of creating and discarding objects repeatedly, you reuse them from a shared pool\u2014saving both CPU time and pressure on the garbage collector.</p> <p>Go\u2019s <code>sync.Pool</code> makes this pattern easy to implement, especially when you\u2019re working with short-lived objects that are created and discarded often. It\u2019s a simple tool that can help smooth out GC behavior and improve throughput under load.</p>"},{"location":"01-common-patterns/object-pooling/#how-object-pooling-works","title":"How Object Pooling Works","text":"<p>Object pooling allows objects to be reused rather than allocated anew, minimizing the strain on the garbage collector. Instead of requesting new memory from the heap each time, objects are fetched from a pre-allocated pool and returned when no longer needed. This reduces allocation overhead and improves runtime efficiency.</p>"},{"location":"01-common-patterns/object-pooling/#using-syncpool-for-object-reuse","title":"Using <code>sync.Pool</code> for Object Reuse","text":""},{"location":"01-common-patterns/object-pooling/#without-object-pooling-inefficient-memory-usage","title":"Without Object Pooling (Inefficient Memory Usage)","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\ntype Data struct {\n    Value int\n}\n\nfunc createData() *Data {\n    return &amp;Data{Value: 42}\n}\n\nfunc main() {\n    for i := 0; i &lt; 1000000; i++ {\n        obj := createData() // Allocating a new object every time\n        _ = obj // Simulate usage\n    }\n    fmt.Println(\"Done\")\n}\n</code></pre> <p>In the above example, every iteration creates a new <code>Data</code> instance, leading to unnecessary allocations and increased GC pressure.</p>"},{"location":"01-common-patterns/object-pooling/#with-object-pooling-optimized-memory-usage","title":"With Object Pooling (Optimized Memory Usage)","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n    \"sync\"\n)\n\ntype Data struct {\n    Value int\n}\n\nvar dataPool = sync.Pool{\n    New: func() any {\n        return &amp;Data{}\n    },\n}\n\nfunc main() {\n    for i := 0; i &lt; 1000000; i++ {\n        obj := dataPool.Get().(*Data) // Retrieve from pool\n        obj.Value = 42 // Use the object\n        dataPool.Put(obj) // Return object to pool for reuse\n    }\n    fmt.Println(\"Done\")\n}\n</code></pre>"},{"location":"01-common-patterns/object-pooling/#pooling-byte-buffers-for-efficient-io","title":"Pooling Byte Buffers for Efficient I/O","text":"<p>Object pooling is especially effective when working with large byte slices that would otherwise lead to high allocation and garbage collection overhead.</p> <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"fmt\"\n    \"sync\"\n)\n\nvar bufferPool = sync.Pool{\n    New: func() any {\n        return new(bytes.Buffer)\n    },\n}\n\nfunc main() {\n    buf := bufferPool.Get().(*bytes.Buffer)\n    buf.Reset()\n    buf.WriteString(\"Hello, pooled world!\")\n    fmt.Println(buf.String())\n    bufferPool.Put(buf) // Return buffer to pool for reuse\n}\n</code></pre> <p>Using <code>sync.Pool</code> for byte buffers significantly reduces memory pressure when dealing with high-frequency I/O operations.</p>"},{"location":"01-common-patterns/object-pooling/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>To prove that object pooling actually reduces allocations and improves speed, we can use Go's built-in memory profiling tools (<code>pprof</code>) and compare memory allocations between the non-pooled and pooled versions. Simulating a full-scale application that actively uses memory for benchmarking is challenging, so we need a controlled test to evaluate direct heap allocations versus pooled allocations.</p> Show the benchmark file <pre><code>package perf\n\nimport (\n    \"sync\"\n    \"testing\"\n)\n\n// Data is a struct with a large fixed-size array to simulate a memory-intensive object.\ntype Data struct {\n    Values [1024]int\n}\n\n// BenchmarkWithoutPooling measures the performance of direct heap allocations.\nfunc BenchmarkWithoutPooling(b *testing.B) {\n    for b.Loop() {\n        data := &amp;Data{}      // Allocating a new object each time\n        data.Values[0] = 42  // Simulating some memory activity\n    }\n}\n\n// dataPool is a sync.Pool that reuses instances of Data to reduce memory allocations.\nvar dataPool = sync.Pool{\n    New: func() any {\n        return &amp;Data{}\n    },\n}\n\n// BenchmarkWithPooling measures the performance of using sync.Pool to reuse objects.\nfunc BenchmarkWithPooling(b *testing.B) {\n    for b.Loop() {\n        obj := dataPool.Get().(*Data) // Retrieve from pool\n        obj.Values[0] = 42            // Simulate memory usage\n        dataPool.Put(obj)             // Return object to pool for reuse\n    }\n}\n</code></pre> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkWithoutPooling-14 1,692,014 705.4 8,192 1 BenchmarkWithPooling-14 160,440,506 7.455 0 0 <p>The benchmark results highlight the performance and memory usage differences between direct allocations and object pooling. The <code>BenchmarkWithoutPooling</code> function demonstrates higher execution time and memory consumption due to frequent heap allocations, resulting in increased garbage collection cycles. A nonzero allocation count confirms that each iteration incurs a heap allocation, contributing to GC overhead and slower performance.</p>"},{"location":"01-common-patterns/object-pooling/#when-should-you-use-syncpool","title":"When Should You Use <code>sync.Pool</code>?","text":"<p> Use sync.Pool when:</p> <ul> <li>You have short-lived, reusable objects (e.g., buffers, scratch memory, request state). Pooling avoids repeated allocations and lets you recycle memory efficiently.</li> <li>Allocation overhead or GC churn is measurable and significant. Reusing objects reduces the number of heap allocations, which in turn lowers garbage collection frequency and pause times.</li> <li>The object\u2019s lifecycle is local and can be reset between uses. When objects don\u2019t need complex teardown and are safe to reuse after a simple reset, pooling is straightforward and effective.</li> <li>You want to reduce pressure on the garbage collector in high-throughput systems. In systems handling thousands of requests per second, pooling helps maintain consistent performance and minimizes GC-related latency spikes.</li> </ul> <p> Avoid sync.Pool when:</p> <ul> <li>Objects are long-lived or shared across multiple goroutines. <code>sync.Pool</code> is optimized for short-lived, single-use objects and doesn\u2019t manage shared ownership or coordination.</li> <li>The reuse rate is low and pooled objects are not frequently accessed. If objects sit idle in the pool, you gain little benefit and may even waste memory.</li> <li>Predictability or lifecycle control is more important than allocation speed. Pooling makes lifecycle tracking harder and may not be worth the tradeoff.</li> <li>Memory savings are negligible or code complexity increases significantly. If pooling doesn\u2019t provide clear benefits, it can add unnecessary complexity to otherwise simple code.</li> </ul>"},{"location":"01-common-patterns/stack-alloc/","title":"Stack Allocations and Escape Analysis","text":"<p>When writing performance-critical Go applications, one of the subtle but significant optimizations you can make is encouraging values to be allocated on the stack rather than the heap. Stack allocations are cheaper, faster, and garbage-free\u2014but Go doesn't always put your variables there automatically. That decision is made by the Go compiler during escape analysis.</p> <p>In this article, we\u2019ll explore what escape analysis is, how to read the compiler\u2019s escape diagnostics, what causes values to escape, and how to structure your code to minimize unnecessary heap allocations. We'll also benchmark different scenarios to show the real-world impact.</p>"},{"location":"01-common-patterns/stack-alloc/#what-is-escape-analysis","title":"What Is Escape Analysis?","text":"<p>Escape analysis is a static analysis performed by the Go compiler to determine whether a variable can be safely allocated on the stack or if it must be moved (\"escape\") to the heap.</p>"},{"location":"01-common-patterns/stack-alloc/#why-does-it-matter","title":"Why does it matter?","text":"<ul> <li>Stack allocations are cheap: the memory is automatically freed when the function returns.</li> <li>Heap allocations are more expensive: they involve garbage collection overhead.</li> </ul> <p>The compiler decides where to place each variable based on how it's used. If a variable can be guaranteed to not outlive its declaring function, it can stay on the stack. If not, it escapes to the heap.</p>"},{"location":"01-common-patterns/stack-alloc/#example-stack-vs-heap","title":"Example: Stack vs Heap","text":"<pre><code>func allocate() *int {\n    x := 42\n    return &amp;x // x escapes to the heap\n}\n\nfunc noEscape() int {\n    x := 42\n    return x // x stays on the stack\n}\n</code></pre> <p>In <code>allocate</code>, <code>x</code> is returned as a pointer. Since the pointer escapes the function, the Go compiler places <code>x</code> on the heap. In <code>noEscape</code>, <code>x</code> is a plain value and doesn\u2019t escape.</p>"},{"location":"01-common-patterns/stack-alloc/#how-to-view-escape-analysis-output","title":"How to View Escape Analysis Output","text":"<p>You can inspect escape analysis with the <code>-gcflags</code> compiler option:</p> <pre><code>go build -gcflags=\"-m\" ./path/to/pkg\n</code></pre> <p>Or for a specific file:</p> <pre><code>go run -gcflags=\"-m\" main.go\n</code></pre> <p>This will print lines like:</p> <pre><code>main.go:10:6: moved to heap: x\nmain.go:14:6: can inline noEscape\n</code></pre> <p>Look for messages like <code>moved to heap</code> to identify escape points.</p>"},{"location":"01-common-patterns/stack-alloc/#what-causes-variables-to-escape","title":"What Causes Variables to Escape?","text":"<p>Here are common scenarios that force heap allocation:</p>"},{"location":"01-common-patterns/stack-alloc/#returning-pointers-to-local-variables","title":"Returning Pointers to Local Variables","text":"<pre><code>func escape() *int {\n    x := 10\n    return &amp;x // escapes\n}\n</code></pre>"},{"location":"01-common-patterns/stack-alloc/#capturing-variables-in-closures","title":"Capturing Variables in Closures","text":"<pre><code>func closureEscape() func() int {\n    x := 5\n    return func() int { return x } // x escapes\n}\n</code></pre>"},{"location":"01-common-patterns/stack-alloc/#interface-conversions","title":"Interface Conversions","text":"<p>When a value is stored in an interface, it may escape:</p> <pre><code>func toInterface(i int) interface{} {\n    return i // escapes if type info needed at runtime\n}\n</code></pre>"},{"location":"01-common-patterns/stack-alloc/#assignments-to-global-variables-or-struct-fields","title":"Assignments to Global Variables or Struct Fields","text":"<pre><code>var global *int\n\nfunc assignGlobal() {\n    x := 7\n    global = &amp;x // escapes\n}\n</code></pre>"},{"location":"01-common-patterns/stack-alloc/#large-composite-literals","title":"Large Composite Literals","text":"<p>Go may allocate large structs or slices on the heap even if they don\u2019t strictly escape.</p> <pre><code>func makeLargeSlice() []int {\n    s := make([]int, 10000) // may escape due to size\n    return s\n}\n</code></pre>"},{"location":"01-common-patterns/stack-alloc/#benchmarking-stack-vs-heap-allocations","title":"Benchmarking Stack vs Heap Allocations","text":"<p>Let\u2019s run a benchmark to explore when heap allocations actually occur\u2014and when they don\u2019t, even if we return a pointer.</p> <pre><code>func StackAlloc() Data {\n    return Data{1, 2, 3} // stays on stack\n}\n\nfunc HeapAlloc() *Data {\n    return &amp;Data{1, 2, 3} // escapes to heap\n}\n\nfunc BenchmarkStackAlloc(b *testing.B) {\n    for b.Loop() {\n        _ = StackAlloc()\n    }\n}\n\nfunc BenchmarkHeapAlloc(b *testing.B) {\n    for b.Loop() {\n        _ = HeapAlloc()\n    }\n}\n</code></pre> <p>Benchmark Results</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkStackAlloc-14 1,000,000,000 0.2604 ns 0 B 0 BenchmarkHeapAlloc-14 1,000,000,000 0.2692 ns 0 B 0 <p>You might expect <code>HeapAlloc</code> to always allocate memory on the heap\u2014but it doesn\u2019t here. That\u2019s because the compiler is smart: in this isolated benchmark, the pointer returned by <code>HeapAlloc</code> doesn\u2019t escape the function in any meaningful way. The compiler can see it\u2019s only used within the benchmark and short-lived, so it safely places it on the stack too.</p>"},{"location":"01-common-patterns/stack-alloc/#forcing-a-heap-allocation","title":"Forcing a Heap Allocation","text":"<pre><code>var sink *Data\n\nfunc HeapAllocEscape() {\n    d := &amp;Data{1, 2, 3}\n    sink = d // d escapes to heap\n}\n\nfunc BenchmarkHeapAllocEscape(b *testing.B) {\n    for b.Loop() {\n        HeapAllocEscape()\n    }\n}\n</code></pre> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkHeapAllocEscape-14 331,469,049 10.55 ns 24 B 1 <p>As shown in <code>BenchmarkHeapAllocEscape</code>, assigning the pointer to a global variable causes a real heap escape. This introduces real overhead: a 40x slower call, a 24-byte allocation, and one garbage-collected object per call.</p> Show the benchmark file <pre><code>package main\n\nimport \"testing\"\n\ntype Data struct {\n    A, B, C int\n}\n\n// heap-alloc-start\nfunc StackAlloc() Data {\n    return Data{1, 2, 3} // stays on stack\n}\n\nfunc HeapAlloc() *Data {\n    return &amp;Data{1, 2, 3} // escapes to heap\n}\n\nfunc BenchmarkStackAlloc(b *testing.B) {\n    for b.Loop() {\n        _ = StackAlloc()\n    }\n}\n\nfunc BenchmarkHeapAlloc(b *testing.B) {\n    for b.Loop() {\n        _ = HeapAlloc()\n    }\n}\n// heap-alloc-end\n\n// escape-start\nvar sink *Data\n\nfunc HeapAllocEscape() {\n    d := &amp;Data{1, 2, 3}\n    sink = d // d escapes to heap\n}\n\nfunc BenchmarkHeapAllocEscape(b *testing.B) {\n    for b.Loop() {\n        HeapAllocEscape()\n    }\n}\n// escape-end\n</code></pre>"},{"location":"01-common-patterns/stack-alloc/#when-to-optimize-for-stack-allocation","title":"When to Optimize for Stack Allocation","text":"<p>Not all escapes are worth preventing. Here\u2019s when it makes sense to focus on stack allocation\u2014and when it\u2019s better to let values escape.</p> <p> When to Avoid Escape</p> <ul> <li>In performance-critical paths. Reducing heap usage in tight loops or latency-sensitive code lowers GC pressure and speeds up execution.</li> <li>For short-lived, small objects. These can be efficiently stack-allocated without involving the garbage collector, reducing memory churn.</li> <li>When you control the full call chain. If the object stays within your code and you can restructure it to avoid escape, it\u2019s often worth the small refactor.</li> <li>If profiling reveals GC bottlenecks. Escape analysis helps you target and shrink memory-heavy allocations identified in real-world traces.</li> </ul> <p> When It\u2019s Fine to Let Values Escape</p> <ul> <li>When returning values from constructors or factories. Returning a pointer from <code>NewThing()</code> is idiomatic Go\u2014even if it causes an escape, it improves clarity and usability.</li> <li>When objects must outlive the function. If you're storing data in a global, sending to a goroutine, or saving it in a struct, escaping is necessary and correct.</li> <li>When allocation size is small and infrequent. If the heap allocation isn\u2019t in a hot path, the benefit of avoiding it is often negligible.</li> <li>When preventing escape hurts readability. Writing awkward code to keep everything on the stack can reduce maintainability for a micro-optimization that won\u2019t matter.</li> </ul>"},{"location":"01-common-patterns/worker-pool/","title":"Goroutine Worker Pools in Go","text":"<p>Go\u2019s lightweight concurrency model makes spawning goroutines nearly free in terms of syntax and initial memory footprint\u2014but that freedom isn\u2019t as cheap as it may seem at first glance. Under high load, unbounded concurrency can quickly lead to excessive memory usage, increased context switching, and unpredictable performance, or even system crashes. A goroutine worker pool introduces controlled parallelism by capping the number of concurrent workers, helping to maintain a balance between resource usage, latency, and throughput.</p>"},{"location":"01-common-patterns/worker-pool/#why-worker-pools-matter","title":"Why Worker Pools Matter","text":"<p>While launching a goroutine for every task is idiomatic and often effective, doing so at scale comes with trade-offs. Each goroutine requires stack space and introduces scheduling overhead. Performance can degrade sharply when the number of active goroutines grows, especially in systems handling unbounded input like HTTP requests, jobs from a queue, or tasks from a channel.</p> <p>A worker pool maintains a fixed number of goroutines that pull tasks from a shared job queue. This creates a backpressure mechanism, ensuring the system never processes more work concurrently than it can handle. Worker pools are particularly valuable when the cost of each task is predictable, and the overall system throughput needs to be stable.</p>"},{"location":"01-common-patterns/worker-pool/#basic-worker-pool-implementation","title":"Basic Worker Pool Implementation","text":"<p>Here\u2019s a minimal implementation of a worker pool:</p> <pre><code>func worker(id int, jobs &lt;-chan int, results chan&lt;- [32]byte) {\n    for j := range jobs {\n        results &lt;- doWork(j)\n    }\n}\n\nfunc doWork(n int) [32]byte {\n    data := []byte(fmt.Sprintf(\"payload-%d\", n))\n    return sha256.Sum256(data)                  // (1)\n}\n\nfunc main() {\n    jobs := make(chan int, 100)\n    results := make(chan [32]byte, 100)\n\n    for w := 1; w &lt;= 5; w++ {\n        go worker(w, jobs, results)\n    }\n\n    for j := 1; j &lt;= 10; j++ {\n        jobs &lt;- j\n    }\n    close(jobs)\n\n    for a := 1; a &lt;= 10; a++ {\n        &lt;-results\n    }\n}\n</code></pre> <ol> <li>Cryptography is for illustration purposes of CPU-bound code</li> </ol> <p>In this example, five workers pull from the <code>jobs</code> channel and push results to the <code>results</code> channel. The worker pool limits concurrency to five tasks at a time, regardless of how many tasks are sent.</p>"},{"location":"01-common-patterns/worker-pool/#worker-count-and-cpu-cores","title":"Worker Count and CPU Cores","text":"<p>The optimal number of workers in a pool is closely tied to the number of CPU cores, which you can obtain in Go using <code>runtime.NumCPU()</code> or <code>runtime.GOMAXPROCS(0)</code>. For CPU-bound tasks\u2014where each worker consumes substantial CPU time\u2014you generally want the number of workers to be equal to or slightly less than the number of logical CPU cores. This ensures maximum core utilization without excessive overhead.</p> <p>If your tasks are I/O-bound (e.g., network calls, disk I/O, database queries), the pool size can be larger than the number of cores. This is because workers will spend much of their time blocked, allowing others to run. In contrast, CPU-heavy workloads benefit from a smaller, tightly bounded pool that avoids contention and context switching.</p>"},{"location":"01-common-patterns/worker-pool/#why-too-many-workers-hurts-performance","title":"Why Too Many Workers Hurts Performance","text":"<p>While adding more workers might seem to increase throughput, this only holds up to a point. Beyond the optimal concurrency level, more workers introduce problems:</p> <ul> <li>Scheduler contention: Go\u2019s runtime has to manage more runnable goroutines than it has CPU cores.</li> <li>Context switching: Excess goroutines create frequent CPU context switches, wasting cycles.</li> <li>Memory pressure: Each goroutine consumes stack space; more workers increase memory usage.</li> <li>Cache thrashing: CPU cache efficiency degrades as goroutines bounce between cores.</li> </ul> <p>This leads to higher latency, more GC activity, and ultimately slower throughput\u2014precisely the opposite of what a well-tuned pool is meant to achieve.</p>"},{"location":"01-common-patterns/worker-pool/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>Worker pools shine in scenarios where the workload is CPU-bound or where concurrency must be capped to avoid saturating a shared resource (e.g., database connections or file descriptors). Benchmarks comparing unbounded goroutine launches vs. worker pools typically show:</p> <ul> <li>Lower peak memory usage</li> <li>More stable response times under load</li> <li>Improved CPU cache locality</li> </ul> Show the benchmark file <pre><code>package perf\n\nimport (\n    // \"log\"\n    \"fmt\"\n    // \"os\"\n    \"runtime\"\n    \"sync\"\n    \"testing\"\n    \"crypto/sha256\"\n)\n\nconst (\n    numJobs     = 10000\n    workerCount = 10\n)\n\nfunc doWork(n int) [32]byte {\n    data := []byte(fmt.Sprintf(\"payload-%d\", n))\n    return sha256.Sum256(data)\n}\n\nfunc BenchmarkUnboundedGoroutines(b *testing.B) {\n    for b.Loop() {\n        var wg sync.WaitGroup\n        wg.Add(numJobs)\n\n        for j := 0; j &lt; numJobs; j++ {\n            go func(job int) {\n                _ = doWork(job)\n                wg.Done()\n            }(j)\n        }\n        wg.Wait()\n    }\n}\n\nfunc worker(jobs &lt;-chan int, wg *sync.WaitGroup) {\n    for job := range jobs {\n        _ = doWork(job)\n        wg.Done()\n    }\n}\n\nfunc BenchmarkWorkerPool(b *testing.B) {\n    for b.Loop() {\n        var wg sync.WaitGroup\n        wg.Add(numJobs)\n\n        jobs := make(chan int, numJobs)\n        for w := 0; w &lt; workerCount; w++ {\n            go worker(jobs, &amp;wg)\n        }\n\n        for j := 0; j &lt; numJobs; j++ {\n            jobs &lt;- j\n        }\n\n        close(jobs)\n        wg.Wait()\n    }\n}\n</code></pre> <p>Results:</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkUnboundedGoroutines-14 2,274 2,499,213 ns 639,350 39,754 BenchmarkWorkerPool-14 3,325 1,791,772 ns 320,707 19,762 <p>In our benchmark, each task performed a CPU-intensive operation (e.g., cryptographic hashing, math, or serialization). With <code>workerCount = 10</code> on an Apple M3 Max machine, the worker pool outperformed the unbounded goroutine model by a significant margin, using fewer resources and completing work faster. Increasing the worker count beyond the number of available cores led to worse performance due to contention.</p>"},{"location":"01-common-patterns/worker-pool/#when-to-use-worker-pools","title":"When To Use Worker Pools","text":"<p> Use a goroutine worker pool when:</p> <ul> <li>You have a large or unbounded stream of incoming work. A pool helps prevent unbounded goroutine growth, which can lead to memory exhaustion and degraded system performance.</li> <li>Processing tasks concurrently can overwhelm system resources. Worker pools provide backpressure and resource control by capping concurrency, helping you avoid CPU thrashing, connection saturation, or I/O overload.</li> <li>You want to limit the number of parallel operations for stability. Controlling the number of active workers reduces the risk of spikes in system load, improving predictability and service reliability under pressure.</li> <li>Tasks are relatively uniform in cost and benefit from queuing. When task sizes are similar, a fixed pool size ensures efficient throughput and fair task distribution without excessive coordination overhead.</li> </ul> <p> Avoid a worker pool when:</p> <ul> <li>Each task must be processed immediately with minimal latency. Queuing in a worker pool introduces delay. For latency-critical tasks, direct goroutine spawning avoids the scheduling overhead.</li> <li>You can rely on Go's scheduler for natural load balancing in low-load scenarios. In light workloads, the overhead of managing a pool may outweigh its benefits. Go\u2019s scheduler can often handle lightweight parallelism efficiently on its own.</li> <li>Workload volume is small and bounded. Spinning up goroutines directly keeps code simpler for limited, predictable workloads without risking uncontrolled growth.</li> </ul>"},{"location":"01-common-patterns/zero-copy/","title":"Zero-Copy Techniques","text":"<p>Managing memory wisely can make a noticeable difference when writing performance-critical Go code. Zero-copy techniques are particularly effective because they avoid unnecessary memory copying by directly manipulating data buffers. By doing so, these techniques significantly enhance throughput and reduce latency, making them highly beneficial for applications that handle intensive I/O operations.</p>"},{"location":"01-common-patterns/zero-copy/#understanding-zero-copy","title":"Understanding Zero-Copy","text":"<p>Traditionally, reading or writing data involves copying between user-space buffers and kernel-space buffers, incurring CPU and memory overhead. Zero-copy techniques bypass these intermediate copying steps, allowing applications to access and process data directly from the underlying buffers. This approach significantly reduces CPU load, memory bandwidth, and latency.</p>"},{"location":"01-common-patterns/zero-copy/#common-zero-copy-techniques-in-go","title":"Common Zero-Copy Techniques in Go","text":""},{"location":"01-common-patterns/zero-copy/#using-ioreader-and-iowriter-interfaces","title":"Using <code>io.Reader</code> and <code>io.Writer</code> Interfaces","text":"<p>Leveraging interfaces such as <code>io.Reader</code> and <code>io.Writer</code> can facilitate efficient buffer reuse and minimize copying:</p> <pre><code>func StreamData(src io.Reader, dst io.Writer) error {\n    buf := make([]byte, 4096) // Reusable buffer\n    _, err := io.CopyBuffer(dst, src, buf)\n    return err\n}\n</code></pre> <p><code>io.CopyBuffer</code> reuses a provided buffer, avoiding repeated allocations and intermediate copies. An in-depth <code>io.CopyBuffer</code> explanation is available on SO.</p>"},{"location":"01-common-patterns/zero-copy/#slicing-for-efficient-data-access","title":"Slicing for Efficient Data Access","text":"<p>Slicing large byte arrays or buffers instead of copying data into new slices is a powerful zero-copy strategy:</p> <pre><code>func process(buffer []byte) []byte {\n    return buffer[128:256] // returns a slice reference without copying\n}\n</code></pre> <p>Slices in Go are inherently zero-copy since they reference the underlying array.</p>"},{"location":"01-common-patterns/zero-copy/#memory-mapping-mmap","title":"Memory Mapping (<code>mmap</code>)","text":"<p>Using memory mapping enables direct access to file contents without explicit read operations:</p> <pre><code>import \"golang.org/x/exp/mmap\"\n\nfunc ReadFileZeroCopy(path string) ([]byte, error) {\n    r, err := mmap.Open(path)\n    if err != nil {\n        return nil, err\n    }\n    defer r.Close()\n\n    data := make([]byte, r.Len())\n    _, err = r.ReadAt(data, 0)\n    return data, err\n}\n</code></pre> <p>This approach maps file contents directly into memory, entirely eliminating copying between kernel and user-space.</p>"},{"location":"01-common-patterns/zero-copy/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>Here's a basic benchmark illustrating performance differences between explicit copying and zero-copy slicing:</p> <pre><code>func BenchmarkCopy(b *testing.B) {\n    data := make([]byte, 64*1024)\n    for b.Loop() {\n        buf := make([]byte, len(data))\n        copy(buf, data)\n    }\n}\n\nfunc BenchmarkSlice(b *testing.B) {\n    data := make([]byte, 64*1024)\n    for b.Loop() {\n        _ = data[:]\n    }\n}\n</code></pre> <p>In <code>BenchmarkCopy</code>, a 64KB buffer is copied into a new slice during every iteration, incurring both memory allocation and data copy overhead. In contrast, <code>BenchmarkSlice</code> simply re-slices the same buffer without any allocation or copying. This demonstrates how zero-copy operations like slicing can vastly outperform traditional copying under load.</p> <p>Info</p> <p>These two functions are not equivalent in behavior\u2014<code>BenchmarkCopy</code> makes an actual deep copy of the buffer, while <code>BenchmarkSlice</code> only creates a new slice header pointing to the same underlying data. This benchmark is not comparing functional correctness but is intentionally contrasting performance characteristics to highlight the cost of unnecessary copying.</p> Benchmark Time per op (ns) Bytes per op Allocs per op BenchmarkCopy 4,246 65536 1 BenchmarkSlice 0.592 0 0"},{"location":"01-common-patterns/zero-copy/#file-io-memory-mapping-vs-standard-read","title":"File I/O: Memory Mapping vs. Standard Read","text":"<p>We also benchmarked file reading performance using <code>os.ReadAt</code> versus <code>mmap.Open</code> for a 4MB binary file.</p> <pre><code>func BenchmarkReadWithCopy(b *testing.B) {\n    f, err := os.Open(\"testdata/largefile.bin\")\n    if err != nil {\n        b.Fatalf(\"failed to open file: %v\", err)\n    }\n    defer f.Close()\n\n    buf := make([]byte, 4*1024*1024) // 4MB buffer\n    for b.Loop() {\n        _, err := f.ReadAt(buf, 0)\n        if err != nil &amp;&amp; err != io.EOF {\n            b.Fatal(err)\n        }\n    }\n}\n\nfunc BenchmarkReadWithMmap(b *testing.B) {\n    r, err := mmap.Open(\"testdata/largefile.bin\")\n    if err != nil {\n        b.Fatalf(\"failed to mmap file: %v\", err)\n    }\n    defer r.Close()\n\n    buf := make([]byte, r.Len())\n    for b.Loop() {\n        _, err := r.ReadAt(buf, 0)\n        if err != nil &amp;&amp; err != io.EOF {\n            b.Fatal(err)\n        }\n    }\n}\n</code></pre> How to run the benchmark <p>To run the benchmark involving <code>mmap</code>, you\u2019ll need to install the required package and create a test file:</p> <pre><code>go get golang.org/x/exp/mmap\nmkdir -p testdata\ndd if=/dev/urandom of=./testdata/largefile.bin bs=1M count=4\n</code></pre> <p>Benchmark Results</p> Benchmark Time per op (ns) Bytes per op Allocs per op ReadWithCopy 94,650 0 0 ReadWithMmap 50,082 0 0 <p>The memory-mapped version (<code>mmap</code>) is nearly 2\u00d7 faster than the standard read call. This illustrates how zero-copy access through memory mapping can substantially reduce read latency and CPU usage for large files.</p> Show the complete benchmark file <pre><code>package perf\n\nimport \"testing\"\n\n\n// interface-start\n\ntype Worker interface {\n    Work()\n}\n\ntype LargeJob struct {\n    payload [4096]byte\n}\n\nfunc (LargeJob) Work() {}\n// interface-end\n\n// bench-slice-start\nfunc BenchmarkBoxedLargeSlice(b *testing.B) {\n    jobs := make([]Worker, 0, 1000)\n    for b.Loop() {\n        jobs = jobs[:0]\n        for j := 0; j &lt; 1000; j++ {\n            var job LargeJob\n            jobs = append(jobs, job)\n        }\n    }\n}\n\nfunc BenchmarkPointerLargeSlice(b *testing.B) {\n    jobs := make([]Worker, 0, 1000)\n    for b.Loop() {\n        jobs := jobs[:0]\n        for j := 0; j &lt; 1000; j++ {\n            job := &amp;LargeJob{}\n            jobs = append(jobs, job)\n        }\n    }\n}\n// bench-slice-end\n\n// bench-call-start\nvar sink Worker\n\nfunc call(w Worker) {\n    sink = w\n}\n\nfunc BenchmarkCallWithValue(b *testing.B) {\n    for b.Loop() {\n        var j LargeJob\n        call(j)\n    }\n}\n\nfunc BenchmarkCallWithPointer(b *testing.B) {\n    for b.Loop() {\n        j := &amp;LargeJob{}\n        call(j)\n    }\n}\n// bench-call-end\n</code></pre>"},{"location":"01-common-patterns/zero-copy/#when-to-use-zero-copy","title":"When to Use Zero-Copy","text":"<p> Zero-copy techniques are highly beneficial for:</p> <ul> <li>Network servers handling large amounts of concurrent data streams. Avoiding unnecessary memory copies helps reduce CPU usage and latency, especially under high load.</li> <li>Applications with heavy I/O operations like file streaming or real-time data processing. Zero-copy allows data to move through the system efficiently without redundant allocations or copies.</li> </ul> <p>Warning</p> <p> Zero-copy should be used judiciously. Since slices share underlying memory, care must be taken to prevent unintended data mutations. Shared memory can lead to subtle bugs if one part of the system modifies data still in use elsewhere. Zero-copy can also introduce additional complexity, so it\u2019s important to measure and confirm that the performance gains are worth the tradeoffs.</p>"},{"location":"01-common-patterns/zero-copy/#real-world-use-cases-and-libraries","title":"Real-World Use Cases and Libraries","text":"<p>Zero-copy strategies aren't just theoretical\u2014they're used in production by performance-critical Go systems:</p> <ul> <li>fasthttp: A high-performance HTTP server designed to avoid allocations. It returns slices directly and avoids <code>string</code> conversions to minimize copying.</li> <li>gRPC-Go: Uses internal buffer pools and avoids deep copying of large request/response messages to reduce GC pressure.</li> <li>MinIO: An object storage system that streams data directly between disk and network using <code>io.Reader</code> without unnecessary buffer replication.</li> <li>Protobuf and MsgPack libraries: Efficient serialization frameworks like <code>google.golang.org/protobuf</code> and <code>vmihailenco/msgpack</code> support decoding directly into user-managed buffers.</li> <li>InfluxDB and Badger: These storage engines use <code>mmap</code> extensively for fast, zero-copy access to database files.</li> </ul> <p>These libraries show how zero-copy techniques help reduce allocations, GC overhead, and system call frequency\u2014all while increasing throughput.</p>"},{"location":"02-networking/","title":"Index","text":"<p>Best practices for connection pooling, buffer reuse, and tuning TCP stack settings for latency-sensitive or throughput-heavy services, validated through network benchmarks and latency percentiles.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/04/03/lazy-initialization-in-go-using-atomics/","title":"Lazy initialization in Go using atomics","text":"<p>Aside from the main performance guide, I'm considering using the blog to share quick, informal insights and quirks related to Go performance and optimizations. Let's see if this casual experiment survives contact with reality.</p> <p>Someone recently pointed out that my <code>getResource()</code> function using atomics has a race condition. Guilty as charged\u2014rookie mistake, really. The issue? I na\u00efvely set the <code>initialized</code> flag to <code>true</code> before the actual resource is ready. Brilliant move, right? This means that with concurrent calls, one goroutine might proudly claim victory while handing out a half-baked resource:</p> <pre><code>var initialized atomic.Bool\nvar resource *MyResource\n\nfunc getResource() *MyResource {\n    if !initialized.Load() {\n        if initialized.CompareAndSwap(false, true) {\n            resource = expensiveInit()\n        }\n    }\n    return resource\n}\n</code></pre> <p>Can this mess be salvaged? Almost certainly, it just needs a touch more thought. To squash the race, we need atomic operations directly on the pointer rather than messing with a separate boolean. Enter Go's atomic package with <code>unsafe.Pointer</code> magic:</p> <pre><code>import (\n    \"sync/atomic\"\n    \"unsafe\"\n)\n\nvar resource unsafe.Pointer // holds *MyResource\n\nfunc getResource() *MyResource {\n    // Attempt to load the resource atomically.\n    ptr := atomic.LoadPointer(&amp;resource)\n    if ptr != nil {\n        return (*MyResource)(ptr) // Resource already initialized, return it\n    }\n\n    // Resource appears uninitialized, perform expensive initialization\n    newRes := expensiveInit()\n\n    // Attempt to atomically set the resource to the newly initialized value\n    if atomic.CompareAndSwapPointer(&amp;resource, nil, unsafe.Pointer(newRes)) {\n        return newRes // Successfully initialized and stored\n    }\n\n    // Another goroutine beat us to initialization, return their initialized resource\n    return (*MyResource)(atomic.LoadPointer(&amp;resource))\n}\n</code></pre> <p>This does the trick\u2014but introduces another subtle hiccup: several goroutines might simultaneously invoke <code>expensiveInit()</code> if they concurrently see a <code>nil</code> pointer. You definetly don't want multiple expensive initializations\u2014unless you're swimming in CPU cycles.</p> <p>So, yes, we do need state tracking. The obvious fix? An intermediate initialization state:</p> <pre><code>import (\n    \"runtime\"\n    \"sync/atomic\"\n    \"unsafe\"\n)\n\nvar resource unsafe.Pointer\nvar initStatus int32 // 0: untouched, 1: in-progress, 2: done\n\nfunc getResource() *MyResource {\n    // Check quickly if initialization is already done\n    if atomic.LoadInt32(&amp;initStatus) == 2 {\n        return (*MyResource)(atomic.LoadPointer(&amp;resource)) // Initialization complete\n    }\n\n    // Attempt to become the goroutine that performs initialization\n    if atomic.CompareAndSwapInt32(&amp;initStatus, 0, 1) {\n        newRes := expensiveInit() // Only this goroutine initializes\n        atomic.StorePointer(&amp;resource, unsafe.Pointer(newRes)) // Store the initialized resource\n        atomic.StoreInt32(&amp;initStatus, 2) // Mark initialization as complete\n        return newRes\n    }\n\n    // Other goroutines wait until initialization completes\n    for atomic.LoadInt32(&amp;initStatus) != 2 {\n        runtime.Gosched() // Chill out and let the initializer finish\n    }\n    return (*MyResource)(atomic.LoadPointer(&amp;resource)) // Initialization complete, return resource\n}\n</code></pre> <p>With this approach, only one goroutine earns the privilege of performing <code>expensiveInit()</code>. Others politely wait, spinning their wheels (well, yielding the CPU politely) until initialization completes.</p> <p>Warning</p> <p>If <code>expensiveInit()</code> panics, this implementation will spin forever! Either handle panic properly or ensure that <code>expensiveInit()</code> has never panicked.</p> <p>Info</p> <p>It's worth noting that this atomic-based approach can be advantageous in scenarios involving a high frequency of calls, where a spinlock's short waiting cycles may be more efficient than a mutex. This is because mutexes can cause frequent context switches, handing control over to the OS scheduler, which can introduce additional overhead.</p> <p>Of course, the more practical solution is usually simpler\u2014<code>sync.Once</code> to the rescue:</p> <pre><code>import \"sync\"\n\nvar (\n    resource *MyResource\n    once     sync.Once\n)\n\nfunc getResource() *MyResource {\n    once.Do(func() {\n        resource = expensiveInit()\n    })\n    return resource\n}\n</code></pre> <p><code>sync.Once</code> elegantly handles initialization, saves your CPUs from unnecessary spin cycles, and keeps your code clean. So, stick to the tried and true unless you have very specific reasons to juggle atomics. Trust me\u2014your future self will thank you.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/atomics/","title":"atomics","text":""}]}